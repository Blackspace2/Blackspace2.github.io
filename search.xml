<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>B样条曲线简介及其python实现</title>
    <url>/2025/01/08/B%E6%A0%B7%E6%9D%A1%E6%9B%B2%E7%BA%BF/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="b样条曲线的计算公式">1. B样条曲线的计算公式</h2>
<p>B 样条曲线 <span class="math inline">\(P(u)\)</span>
可以表示为控制点和基函数的加权和： <span class="math display">\[
P(u)=\sum_{i=0}^{n} {p_i B_{i,k}(u)} \ ,u\in \left[ u_{k-1}, u_{k+1}
\right]
\tag{1}
\]</span></p>
<h2 id="符号解释">2. 符号解释</h2>
<h3 id="pu">2.1 <span class="math inline">\(P(u)\)</span></h3>
<p>B 样条曲线上的点，也是我们所要求的结果。</p>
<h3 id="p_i">2.2 <span class="math inline">\(p_i\)</span></h3>
<p>控制点， <span class="math inline">\(i=0,1,2,..,n\)</span>，一共有
<span class="math inline">\(n+1\)</span> 个。</p>
<h3 id="b_ik">2.3 <span class="math inline">\(B_{i,k}\)</span></h3>
<p>基函数</p>
<h3 id="u">2.4 <span class="math inline">\(u\)</span> <span
id="2.4"></span></h3>
<p>相当于自变量，有效区间为 <span class="math inline">\(\left[ u_{k-1},
u_{k+1} \right]\)</span>。</p>
<h3 id="k">2.5 <span class="math inline">\(k\)</span></h3>
<p>B
样条曲线的<strong>次数</strong>，这里要注意次数和阶数的区别和联系：次数
= 阶数 - 1。</p>
<p>对于 B 样条的次数 <span
class="math inline">\(k\)</span>，必须满足：<span id="(2)"></span> <span
class="math display">\[k = m - n - 1 \tag{2}\]</span> 其中 <span
class="math inline">\(m\)</span><span id="m"></span> 是 <strong>节点
(knots)</strong> 将 B 样条曲线划分的段数，<span
class="math inline">\(n\)</span> 为控制点的个数减一。</p>
<p>上面说的节点就是划分 B
样条的比例，由节点组成的一组向量就成为节点矢量，例如
<code>[0, 0.2, 0.4, 0.6, 0.8, 1]</code>。不同的节点矢量进而产生了不同的
B 样条种类，例如均匀 B 样条、准均匀 B 样条、分段 B 样条以及非均匀 B
样条等等。</p>
<h2 id="基函数的计算">3. 基函数的计算</h2>
<p>de Boor-Cox递归方法<br />
<span class="math display">\[
B_{i,k}(u)=\frac{u-u_i}{u_{i+1}-u_i}B_{i,k-1}(u)+\frac{u_{i+k}-u}{u_{i+k}-u_{i+1}}B_{i+1,k-1}(u)
\tag{3}
\]</span></p>
<p>这里需要给出说明，不带下标的 <span class="math inline">\(u\)</span>
指的是 <a href="#2.4">2.4</a> 中的 <span
class="math inline">\(u\)</span>，而带下标则指的是节点矢量。</p>
<p>在代码实现中，我们规定 <span
class="math inline">\(\frac{0}{0}=0\)</span>。</p>
<h2 id="节点矢量-knots-的计算">4. 节点矢量 (knots) 的计算</h2>
<p>节点矢量的取值可以是在 0 到 1
之间，也可以是其他范围，但是在代码是实现的时候一定要注意前后保持一致。<br />
本文使用节点矢量取值为 0 到 1，这样也可以表示比例嘛。</p>
<p>这里需要注意节矢量的长度，也就是 <span
class="math inline">\(m\)</span>，通过式 <a href="#(2)">(2)</a>
可知，<span class="math inline">\(m=k+n+1\)</span>。</p>
<h3 id="均匀节点-uniform-node">4.1 均匀节点 (uniform node)</h3>
<p>显然，节点的分布是均匀的，故从 0 到
1按照节点矢量的长度均匀划分即可。<br />
（因为要将曲线划分为 <a href="#m">m</a> 段，当然需要 m+1个点）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">uniform_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, n + k + <span class="number">2</span>)  <span class="comment"># m = n + k + 1</span></span><br></pre></td></tr></table></figure>
<p>均匀 B 样条曲线不一定过首尾控制点，并且在图像上是闭合的</p>
<h3 id="准均匀节点-quasi-uniform-node">4.2 准均匀节点 (quasi uniform
node)</h3>
<p>准均匀节点可以目的在于对曲线的端点进行行为控制，通过设计节点矢量，使得生成的
B 样条曲线经过首尾控制点。<br />
<span class="math inline">\(k\)</span>
次准均匀节点矢量中，两端节点具有重复度 <span
class="math inline">\(k+1\)</span>，所有内节点呈现均匀分布。<br />
在代码实现中，我们可以让节点矢量首尾分别为 <span
class="math inline">\(k\)</span> 个 0 和 1，然后中间 <span
class="math inline">\(n-k+2\)</span> 为 0 到 1 的均匀分布就行了，即
<span class="math display">\[ \nonumber
\left[ \begin{matrix}
\underset{k}{\underbrace{0,0,...,0}},\
\underset{n-k+2}{\underbrace{0,...,1}},\
\underset{k}{\underbrace{1,1,...,1}}\\
\end{matrix} \right]
\]</span></p>
<figure class="highlight python"><figcaption><span>python</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">quasi_uniform_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line">    mid = np.linspace(<span class="number">0</span>, <span class="number">1</span>, n - k + <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.r_[np.zeros(k), mid, np.ones(k)]  <span class="comment"># 拼接</span></span><br></pre></td></tr></table></figure>
<h3 id="分段节点-piecewise-node">4.3 分段节点 (piecewise node)</h3>
<p>基于该节点矢量的 B 样样条曲线又称为分段 Bezier
曲线，是一组顺序首尾相接且同为 <span class="math inline">\(k\)</span>
次的 Bezier 曲线。<br />
<span class="math inline">\(k\)</span>
次的分段节点矢量中，首末端节点重复度依旧为 <span
class="math inline">\(k+1\)</span>，内节点重复度为 <span
class="math inline">\(k\)</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">piecewise_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## restricted condition</span></span><br><span class="line">    <span class="keyword">assert</span> (n - k) % k == <span class="number">0</span>, <span class="string">&quot;input is valid.&quot;</span></span><br><span class="line"></span><br><span class="line">    tmp = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="built_in">int</span>((n - k) / k + <span class="number">2</span>))</span><br><span class="line">    mid = np.r_[tmp[<span class="number">0</span>], np.repeat(tmp[<span class="number">1</span>:-<span class="number">1</span>], k), tmp[-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> np.r_[np.zeros(k), mid, np.ones(k)]</span><br></pre></td></tr></table></figure>
<p>需要注意 <span class="math inline">\(n-k\)</span> 必须是 <span
class="math inline">\(k\)</span> 的整数倍，否则不能生成曲线。</p>
<h3 id="非均匀节点-non-uniform-node">4.4 非均匀节点 (non-uniform
node)</h3>
<p>Hartley-Judd 算法 首尾重合度为 <span
class="math inline">\(k+1\)</span>，内节点定义为：</p>
<p><span class="math display">\[
\begin{cases}
t_k=0\\
t_i=\sum_{j=k+1}^i{\bigl( t_j-t_{j-1} \bigr)} \\
t_{n+1}=1\\
\end{cases}
\tag{4}
\]</span> <span class="math display">\[
t_i-t_{i-1}=\frac{\sum_{j=i-k}^{i-1}{l_j}}{\sum_{i=k+1}^{n+1}{\sum_{j=i-k}^{i-1}{l_j}}}
\tag{5}
\]</span></p>
<figure class="highlight python"><figcaption><span>python</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">non_uniform_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    l = np.sqrt(np.<span class="built_in">sum</span>(np.diff(control_points, axis=<span class="number">0</span>) ** <span class="number">2</span>, axis=<span class="number">1</span>))</span><br><span class="line">    ll = l[<span class="number">0</span> : <span class="built_in">len</span>(l) - <span class="number">1</span>] + l[<span class="number">1</span>::]</span><br><span class="line">    L = np.<span class="built_in">sum</span>(ll)</span><br><span class="line"></span><br><span class="line">    mid_size = n - k</span><br><span class="line">    mid = np.zeros(mid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(mid_size):</span><br><span class="line">        mid[i] = np.<span class="built_in">sum</span>(ll[<span class="number">0</span> : i + <span class="number">1</span>]) / L</span><br><span class="line"></span><br><span class="line">    knots = np.r_[np.zeros(k + <span class="number">1</span>), mid, np.ones(k + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> knots</span><br></pre></td></tr></table></figure>
<h2 id="python-实现">5. python 实现</h2>
<figure class="highlight python"><figcaption><span>python</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">uniform_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, n + k + <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quasi_uniform_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line">    mid = np.linspace(<span class="number">0</span>, <span class="number">1</span>, n - k + <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.r_[np.zeros(k), mid, np.ones(k)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">piecewise_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## restricted condition</span></span><br><span class="line">    <span class="keyword">assert</span> (n - k) % k == <span class="number">0</span>, <span class="string">&quot;input is valid.&quot;</span></span><br><span class="line"></span><br><span class="line">    tmp = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="built_in">int</span>((n - k) / k + <span class="number">2</span>))</span><br><span class="line">    mid = np.r_[tmp[<span class="number">0</span>], np.repeat(tmp[<span class="number">1</span>:-<span class="number">1</span>], k), tmp[-<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.r_[np.zeros(k), mid, np.ones(k)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">non_uniform_node</span>(<span class="params">control_points, k=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Hartley-Judd algorithem</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num, _ = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    l = np.sqrt(np.<span class="built_in">sum</span>(np.diff(control_points, axis=<span class="number">0</span>) ** <span class="number">2</span>, axis=<span class="number">1</span>))</span><br><span class="line">    ll = l[<span class="number">0</span> : <span class="built_in">len</span>(l) - <span class="number">1</span>] + l[<span class="number">1</span>::]</span><br><span class="line">    L = np.<span class="built_in">sum</span>(ll)</span><br><span class="line"></span><br><span class="line">    mid_size = n - k</span><br><span class="line">    mid = np.zeros(mid_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(mid_size):</span><br><span class="line">        mid[i] = np.<span class="built_in">sum</span>(ll[<span class="number">0</span> : i + <span class="number">1</span>]) / L</span><br><span class="line"></span><br><span class="line">    knots = np.r_[np.zeros(k + <span class="number">1</span>), mid, np.ones(k + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> knots</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_B</span>(<span class="params">i, k, knots, u</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    de Boor-Cox recursion</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        i (int): ith point idx</span></span><br><span class="line"><span class="string">        k (int): degree of b-spline , equal to ord - 1</span></span><br><span class="line"><span class="string">        knots (ndarray): 1 dim</span></span><br><span class="line"><span class="string">        u : independent variable</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        B: B_&#123;i,k&#125;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">        B = <span class="number">1</span> <span class="keyword">if</span> knots[i] &lt;= u &lt;= knots[i + <span class="number">1</span>] <span class="keyword">else</span> <span class="number">0</span>  <span class="comment">##</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        coef1 = coef2 = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> knots[i + k - <span class="number">1</span>] - knots[i] != <span class="number">0</span>:</span><br><span class="line">            coef1 = (u - knots[i]) / (knots[i + k - <span class="number">1</span>] - knots[i])</span><br><span class="line">        <span class="keyword">if</span> knots[i + k] - knots[i + <span class="number">1</span>] != <span class="number">0</span>:</span><br><span class="line">            coef2 = (knots[i + k] - u) / (knots[i + k] - knots[i + <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        B = coef1 * cal_B(i, k - <span class="number">1</span>, knots, u) + coef2 * cal_B(i + <span class="number">1</span>, k - <span class="number">1</span>, knots, u)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> B</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_curve</span>(<span class="params">control_points, knots, t</span>):</span><br><span class="line"></span><br><span class="line">    num, dims = control_points.shape</span><br><span class="line">    n = num - <span class="number">1</span></span><br><span class="line">    k = <span class="built_in">len</span>(knots) - n - <span class="number">1</span>  <span class="comment"># degree of b-spline</span></span><br><span class="line"></span><br><span class="line">    N = <span class="built_in">len</span>(t)</span><br><span class="line">    P = np.zeros((N, dims))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        u = t[idx]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num):</span><br><span class="line">            P[idx, :] += control_points[i, :] * cal_B(i, k, knots, u)</span><br><span class="line">    <span class="keyword">return</span> P</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 体验 https://superjerryshen.github.io/b-spline-demos/##/uniform-b-spline-of-order-3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">    control_points = np.array([[<span class="number">50</span>, <span class="number">50</span>], [<span class="number">100</span>, <span class="number">300</span>], [<span class="number">300</span>, <span class="number">100</span>], [<span class="number">380</span>, <span class="number">200</span>], [<span class="number">400</span>, <span class="number">600</span>], [<span class="number">500</span>, <span class="number">400</span>], [<span class="number">300</span>, <span class="number">600</span>]])</span><br><span class="line">    </span><br><span class="line">    N = <span class="number">500</span></span><br><span class="line">    t = np.linspace(<span class="number">0.0</span>, <span class="number">1.0</span>, N)</span><br><span class="line"></span><br><span class="line">    uniform_knots = uniform_node(control_points)</span><br><span class="line">    quasi_knots = quasi_uniform_node(control_points)</span><br><span class="line">    piecewise_knots = piecewise_node(control_points)</span><br><span class="line">    non_knots = non_uniform_node(control_points)</span><br><span class="line"></span><br><span class="line">    P_uniform = cal_curve(control_points, uniform_knots, t)</span><br><span class="line">    P_quasi = cal_curve(control_points, quasi_knots, t)</span><br><span class="line">    P_piecewise = cal_curve(control_points, piecewise_knots, t)</span><br><span class="line">    P_non = cal_curve(control_points, non_knots, t)</span><br><span class="line"></span><br><span class="line">    fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 均匀 会闭合</span></span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">0</span>].scatter(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, facecolors=<span class="string">&quot;none&quot;</span>, label=<span class="string">&quot;control points&quot;</span>)</span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">0</span>].plot(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, linewidth=<span class="number">0.2</span>)</span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">0</span>].plot(P_uniform[:, <span class="number">0</span>], P_uniform[:, <span class="number">1</span>], color=<span class="string">&quot;C3&quot;</span>, label=<span class="string">&quot;uniform&quot;</span>)</span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">0</span>].legend()</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 准均匀</span></span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">1</span>].scatter(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, facecolors=<span class="string">&quot;none&quot;</span>, label=<span class="string">&quot;control points&quot;</span>)</span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">1</span>].plot(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, linewidth=<span class="number">0.2</span>)</span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">1</span>].plot(P_quasi[:, <span class="number">0</span>], P_quasi[:, <span class="number">1</span>], color=<span class="string">&quot;C3&quot;</span>, label=<span class="string">&quot;quasi&quot;</span>)</span><br><span class="line">    axs[<span class="number">0</span>, <span class="number">1</span>].legend()</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 分段</span></span><br><span class="line">    axs[<span class="number">1</span>, <span class="number">0</span>].scatter(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, facecolors=<span class="string">&quot;none&quot;</span>, label=<span class="string">&quot;control points&quot;</span>)</span><br><span class="line">    axs[<span class="number">1</span>, <span class="number">0</span>].plot(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, linewidth=<span class="number">0.2</span>)</span><br><span class="line">    axs[<span class="number">1</span>, <span class="number">0</span>].plot(P_piecewise[:, <span class="number">0</span>], P_piecewise[:, <span class="number">1</span>], color=<span class="string">&quot;C3&quot;</span>, label=<span class="string">&quot;piecewise&quot;</span>)</span><br><span class="line">    axs[<span class="number">1</span>, <span class="number">0</span>].legend()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## HJ 非均匀</span></span><br><span class="line">    axs[<span class="number">1</span>,<span class="number">1</span>].scatter(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, facecolors=<span class="string">&quot;none&quot;</span>, label=<span class="string">&quot;control points&quot;</span>)</span><br><span class="line">    axs[<span class="number">1</span>,<span class="number">1</span>].plot(control_points[:, <span class="number">0</span>], control_points[:, <span class="number">1</span>], color=<span class="string">&quot;C0&quot;</span>, linewidth=<span class="number">0.2</span>)</span><br><span class="line">    axs[<span class="number">1</span>,<span class="number">1</span>].plot(P_non[:, <span class="number">0</span>], P_non[:, <span class="number">1</span>], color=<span class="string">&quot;C3&quot;</span>, label=<span class="string">&quot;non uniform&quot;</span>)</span><br><span class="line">    axs[<span class="number">1</span>,<span class="number">1</span>].legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>效果：<br />
<img src="BSpline.png" alt="BSpline" /></p>
<p>todo：<br />
很诡异的一点，在 <code>cal_curve</code> 函数中计算 <span
class="math inline">\(k\)</span> 的时候，满足 <span
class="math inline">\(k=m-n-1\)</span>
的时候，后三种样条曲线都不经过首尾控制点，这是不符合预期的，但是用 <span
class="math inline">\(k = (m+1) - n - 1\)</span>
的时候，却和正常预期的结果一样，不知道是哪里出问题了。<br />
有空了再来填补空缺</p>
<h2 id="参考">6. 参考</h2>
<p><strong>[1]</strong> <a
href="https://b23.tv/e93CqZR">计算机图形学-中国农大-赵明-B站
8.5.1~8.6.2</a><br />
<strong>[2]</strong> <a
href="https://blog.csdn.net/deepsprings/article/details/107828889?spm=1001.2014.3001.5506">详解样条曲线-CSDN</a><br />
<strong>[3]</strong> <a
href="https://zhuanlan.zhihu.com/p/686518292">B样条曲线和Nurbs曲线
图文并茂的理解节点和节点区间-知乎</a><br />
<strong>[4]</strong>《计算几何算法与实现》—— 孔令德</p>
]]></content>
      <categories>
        <category>计算机图形学</category>
        <category>B样条</category>
      </categories>
      <tags>
        <tag>B样条</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 中的 markdown 语法</title>
    <url>/2025/01/07/Hexo%E4%B8%AD%E7%9A%84md%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="页内跳转">1. 页内跳转</h2>
<p>跳转到地方后面加上
<code>&lt;span id="jump"&gt;&lt;/span&gt;</code><br />
需要跳转的地方 <code>[跳转到](#jump)</code><br />
注：<code>jump</code> 字段不能有中文、空格</p>
<h2 id="博客嵌入图片">2. 博客嵌入图片</h2>
<p>在 Hexo 的 <code>_config.yml</code> 文件下设置：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">post_asset_folder:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">marked:</span></span><br><span class="line">  <span class="attr">prependRoot:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">postAsset:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>此后当新建 <code>.md</code>
文件时，会在同级目录下自动生成一个同名文件夹，里面课以用于存放附件（如图片等）。<br />
此时图片引用语法： <code>![文本](图片名字)</code></p>
<h2 id="html-插入图片">3. html 插入图片</h2>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">figure</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;图片路径&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;说明&quot;</span> <span class="attr">width</span>=<span class="string">&quot;像素或者百分比&quot;</span> <span class="attr">height</span>=<span class="string">&quot;像素或者百分比&quot;</span> <span class="attr">loading</span>=<span class="string">&quot;lazy&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">figcaption</span>&gt;</span>图例<span class="tag">&lt;/<span class="name">figcaption</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">figure</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="数学公式以及编号问题">4. 数学公式以及编号问题</h2>
<p>渲染引擎：MathJax<br />
插件：hexo-renderer-pandoc<br />
参数（位于<code>_config.next.yml</code>）：<code>mathjax.tags = all</code></p>
<p>写行间公式的时候，两个 <code>$$</code>
一定要分别紧靠开头和结尾，并且中间不能有空行，否则在网页上会看见开头和结尾的两个
<code>$$</code> 符号（虽然公式可以正常渲染出来）。</p>
<p>在上述环境下，hexo 会自动给每个公式进行编号，当然也可以用
<code>\tag&#123;&#125;</code> 自己设置编号。如果使用 <code>\tag&#123;&#125;</code>
自定义编号，那么 hexo
依然会跳过这个公式并且对后面的公式进行自动编号。<br />
当公式不需要编号的时候，可以用 <code>\nonumber</code> 或
<code>\notag</code> 来标记；此外，也可以使用 带 <code>*</code> 的
<code>equation</code> 环境来取消对公式的编号。</p>
<p>对于公式引用，Next 官方文档中提到可以用 <code>\label&#123;&#125;</code>
和对应的 <code>\eqref&#123;&#125;</code> 来进行相应的公式引用。</p>
<p>参考：<br />
[1] <a
href="https://theme-next.js.org/docs/third-party-services/math-equations">Next
官方文档</a><br />
[2] <a
href="https://docs.mathjax.org/en/latest/input/tex/eqnumbers.html">MathJax
公式编号说明</a></p>
]]></content>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>git 命令思维导图</title>
    <url>/2025/01/17/git-%E5%91%BD%E4%BB%A4%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<figure>
<img src="git.jpg" alt="git 命令思维导图" />
<figcaption aria-hidden="true">git 命令思维导图</figcaption>
</figure>
<p>来源：<a
href="https://github.com/toypipi/graph_bed/blob/master/image/20200810/git%E5%91%BD%E4%BB%A4%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.jpg">github</a></p>
]]></content>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>python 常用语法糖</title>
    <url>/2025/01/17/python-%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%E7%B3%96/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="数字分隔符">1. 数字分隔符</h2>
<p>当数值较大时，可以用数字分隔符来进行划分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">1_000_000</span> <span class="comment"># 等同于 1000000</span></span><br></pre></td></tr></table></figure>
<h2 id="交换变量值">2. 交换变量值</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a, b = b, a</span><br></pre></td></tr></table></figure>
<h2 id="连续比较式">3. 连续比较式</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="number">10</span> &lt; a &lt; <span class="number">20</span>:</span><br><span class="line">    <span class="comment"># do something</span></span><br></pre></td></tr></table></figure>
<h2 id="字符串乘法">4. 字符串乘法</h2>
<p>可用于快速构造字符串</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<h2 id="列表拼接">5. 列表拼接</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">c = a + b  <span class="comment"># c = [1, 2, 3, 4, 5, 6]</span></span><br><span class="line"><span class="comment"># a.extend(b) 后相当于 a = a + b = [1, 2, 3, 4, 5, 6], a 被改变了</span></span><br><span class="line"><span class="comment"># 不能用一个变量去接收，即 c = a.extend(b) 这样 c 的值是 None，并不会赋值给 c </span></span><br></pre></td></tr></table></figure>
<h2 id="打包解包">6. 打包解包</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">x, y, z = a</span><br></pre></td></tr></table></figure>
<h2 id="with-语句">7. with 语句</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;file_name&quot;</span>,<span class="string">&#x27;mode&#x27;</span>) <span class="keyword">as</span> new_name:</span><br><span class="line">    <span class="comment"># do something</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>语法糖</tag>
      </tags>
  </entry>
  <entry>
    <title>python 装饰器</title>
    <url>/2025/01/17/python-%E8%A3%85%E9%A5%B0%E5%99%A8/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="装饰器简介">1. 装饰器简介</h2>
<h3 id="函数装饰函数">1.1 函数装饰函数</h3>
<p>python 中的装饰器也是<strong>语法糖</strong>的一种。<br />
通常可以把装饰器理解为输入和输出都是函数的函数。
函数调用函数返回一个函数 (或者说 callab)。</p>
<p>装饰器的一大特性是，能把被装饰的函数替换成其他函数；第二个特性是，装饰器在加载模块时立即执行，函数装饰器在导入模块时立即执行，而被装饰的函数只在明确调用时运行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是一个计时的装饰器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timeit</span>(<span class="params">f</span>):</span><br><span class="line">    <span class="comment"># 使用 *args, **kwargs 允许函数接受变长的参数</span></span><br><span class="line">    <span class="comment"># 就是说你这个函数来了什么参数 我都能接受</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        start = time.time()</span><br><span class="line">        ret = f(*args, **kwargs)</span><br><span class="line">        <span class="built_in">print</span>(time.time() - start)</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">myfunc</span>(<span class="params">x</span>):</span><br><span class="line">    time.sleep(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于下面这种写法</span></span><br><span class="line"><span class="comment"># def myfunc(x):</span></span><br><span class="line"><span class="comment">#     time.sleep(x)</span></span><br><span class="line"><span class="comment"># myfunc = timeit(myfunc)</span></span><br></pre></td></tr></table></figure>
<p>带参数的装饰器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">timeit</span>(<span class="params">iteration</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>(<span class="params">f</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            start = time.time()</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iteration):</span><br><span class="line">                ret = f(*args, **kwargs)</span><br><span class="line">            <span class="built_in">print</span>(time.time() - start)</span><br><span class="line">            <span class="keyword">return</span> ret</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@timeit(<span class="params"><span class="number">10</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">double</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面等价于：</span></span><br><span class="line"><span class="comment"># def double(x):</span></span><br><span class="line"><span class="comment">#     return x * 2</span></span><br><span class="line"><span class="comment"># double = timeit(10)(double)</span></span><br></pre></td></tr></table></figure>
<p>这里 <code>timeit(10)</code>
的返回值也是一个函数，并且这个函数的输入和输出都是函数</p>
<h3 id="类装饰函数">1.2 类装饰函数</h3>
<p>类调用一个函数返回一个 object，依然是 callable</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func</span>):</span><br><span class="line">        <span class="variable language_">self</span>.func = func</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 变为 callable 的</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        start = time.time()</span><br><span class="line">        ret = <span class="variable language_">self</span>.func(*args, **kwargs)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Time: <span class="subst">&#123;time.time() - start&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="meta">@Timer</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于</span></span><br><span class="line"><span class="comment"># def add(a, b):</span></span><br><span class="line"><span class="comment">#     return a + b</span></span><br><span class="line"><span class="comment"># add = Timer(add)</span></span><br><span class="line"><span class="comment"># 相当于把一个函数变为了一个类的对象</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(add(<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>带参数的类作为装饰器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, prefix</span>):</span><br><span class="line">        <span class="variable language_">self</span>.prefix = prefix</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 变为 callable 的</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, prefix</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">            start = time.time()</span><br><span class="line">            ret = <span class="variable language_">self</span>.func(*args, **kwargs)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;self.prefix&#125;</span><span class="subst">&#123;time.time() - start&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> ret</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@Timer(<span class="params">prefix=<span class="string">&quot;cost_time:&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于</span></span><br><span class="line"><span class="comment"># def add(a, b):</span></span><br><span class="line"><span class="comment">#     return a + b</span></span><br><span class="line"><span class="comment"># add = Timer(prefix=&quot;cost_time&quot;)(add)</span></span><br><span class="line"><span class="comment"># 相当于把一个函数变为了一个类的对象</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(add(<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<h3 id="函数装饰类">1.3 函数装饰类</h3>
<p>装饰器去装饰类</p>
<p>当打印一个自定义类的时候，不会输出任何有价值的内容，只会告诉你这是一个人什么类，除非重载他的
<code>__str__</code> 方法来改变他的 <code>print</code>
结果，但是每次都重载很麻烦，所以可以写一个装饰器来装饰类用于重载其
<code>__str__</code> 方法。下面就是一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_str</span>(<span class="params">cls</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(<span class="variable language_">self</span>.__dict__)</span><br><span class="line">    cls.__str__ = __str__</span><br><span class="line">    <span class="keyword">return</span> cls</span><br><span class="line"></span><br><span class="line"><span class="meta">@add_str</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyObject</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="variable language_">self</span>.a = a</span><br><span class="line">        <span class="variable language_">self</span>.b = b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于：</span></span><br><span class="line"><span class="comment"># class MyObject:</span></span><br><span class="line"><span class="comment">#     def __init__(self, a, b):</span></span><br><span class="line"><span class="comment">#         self.a = a</span></span><br><span class="line"><span class="comment">#         self.b = b</span></span><br><span class="line"><span class="comment"># add_str(MyObject)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    obj = MyObject(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure>
<h3 id="类装饰类">1.4 类装饰类？</h3>
<p>emmm，本质上其实还是函数，是一个输入和输出都是类的函数</p>
<h3 id="小结">1.5 小结</h3>
<p>其实只要转化为等价形式就知道装饰器在干什么了，因为装饰器的本质毕竟还是语法糖。</p>
<h2 id="一些常见的装饰器">2. 一些常见的装饰器</h2>
<h3 id="staticmethod">2.1 <span class="citation"
data-cites="staticmethod">@staticmethod</span></h3>
<p>用于定义静态方法，意味着被它装饰的函数不需要创建类的实例（不需要传递
<code>self</code> 参数）就可以直接调用，即变为静态函数。</p>
<h3 id="classmethod">2.2 <span class="citation"
data-cites="classmethod">@classmethod</span></h3>
<p>用于定义类的方法，被它装饰的函数第一个参数应该为类 <code>cls</code>
，变为类的方法，但不能访问实例的属性，因为传入的是类本身
<code>cls</code> ，而不是实例 <code>self</code> 。</p>
<p>类方法可以访问类的属性和其他类方法，但不能访问实例的属性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span>:</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">static_method</span>(<span class="params">x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> x + y</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">class_method</span>(<span class="params">cls, x, y</span>):</span><br><span class="line">        <span class="keyword">return</span> x * y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(MyClass.static_method(<span class="number">2</span>, <span class="number">3</span>))  <span class="comment"># 输出: 5</span></span><br><span class="line"><span class="built_in">print</span>(MyClass.class_method(<span class="number">2</span>, <span class="number">3</span>))   <span class="comment"># 输出: 6</span></span><br></pre></td></tr></table></figure>
<h3 id="property">2.3 <span class="citation"
data-cites="property">@property</span></h3>
<p>用于将一个方法转化为属性，可以让你在访问属性时调用一个函数，从而封装对属性的访问。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, radius</span>):</span><br><span class="line">        <span class="variable language_">self</span>._radius = radius   <span class="comment"># 私有属性</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">radius</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._radius  <span class="comment"># 只读属性     </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># @&lt;property_name&gt;.setter 是一个装饰器，</span></span><br><span class="line">    <span class="comment"># 用于为 @property 装饰的属性提供设置器即定义属性的写操作。</span></span><br><span class="line">    <span class="comment"># 通过这个装饰器，你可以指定如何修改某个属性的值，从而让这个属性变为可写的。</span></span><br><span class="line"><span class="meta">    @radius.setter</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">radius</span>(<span class="params">self, value</span>):</span><br><span class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Radius cannot be negative&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>._radius = value</span><br><span class="line"></span><br><span class="line">c = Circle(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(c.radius)  <span class="comment"># 输出: 5</span></span><br><span class="line">c.radius = <span class="number">10</span>    <span class="comment"># 设置 radius</span></span><br><span class="line"><span class="built_in">print</span>(c.radius)  <span class="comment"># 输出: 10</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title>主成分分析-PCA</title>
    <url>/2025/01/18/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="概要">1. 概要</h2>
<p>主成分分析 (principal component analysis,PCA)</p>
<ul>
<li>无监督学习方法</li>
<li><strong>线性</strong>降维</li>
<li>数据压缩</li>
</ul>
<h2 id="推导">2. 推导</h2>
<h3 id="符号说明">2.1. 符号说明</h3>
<ul>
<li>数据 <span class="math inline">\(X\in \mathbb{R}^{n\times
p}\)</span>，<span class="math inline">\(n\)</span> 是样本个数，<span
class="math inline">\(p\)</span> 是特征个数
<ul>
<li>数据需要<strong>预处理</strong>：<strong>去中心化</strong>，每个样本减去均值即可</li>
</ul></li>
<li>投影矩阵 <span class="math inline">\(W \in \mathbb{R}^{p\times
k}\)</span>，<span class="math inline">\(p\)</span> 同上，<span
class="math inline">\(k\)</span> 是降维后特征个数</li>
<li>矩阵 <span class="math inline">\(A\)</span> 的 <span
class="math inline">\(F\)</span> 范数 <span
class="math display">\[\|A\|_{F} =
\sqrt{\sum_{i}\sum_j|a_{ij}|^2}=\sqrt{\mathrm{Tr}(A^TA)}\nonumber\]</span></li>
</ul>
<h3 id="最小重构误差">2.2. 最小重构误差</h3>
<p>重投影的数据和原始数据之间的误差最小，从而丢失的信息少。<br />
数学描述： <span class="math display">\[
\begin{array}{c}
\underset{W}{\min }\ \|X-XWW^T\|_{F}^{2}\\
\text{s.t.}\quad W^TW=I_k
\end{array}
\]</span></p>
<h3 id="最大可分性">2.3. 最大可分性</h3>
<p>数据经过投影矩阵变换到超平面上之后的数据尽可能分开，即投影后数据的方差尽可能的大，从而保留的信息多。<br />
数学描述： <span class="math display">\[
\begin{array}{c}
\underset{W}{\max }\ \|XW\|_{F}^{2}\\
\text{s.t.}\quad W^TW=I_k
\end{array}
\]</span></p>
<h3 id="最小重构和最大可分等价">2.4. 最小重构和最大可分等价</h3>
<p><span class="math display">\[\label{优化}
\begin{aligned}
\underset{W}{\min }\|X-XWW^T\|_{F}^{2}
&amp;=\underset{W}{\min } \
\mathrm{Tr}\left(\left(X-XWW^T\right)^T\left(X-XWW^T\right)\right)\\
&amp;={\small \underset{W}{\min } \
\mathrm{Tr}\left(X^TX-X^TXWW^T-WW^TX^TX+WW^TX^TXWW^T \right)}\\
&amp;=\underset{W}{\min } \ \mathrm{Tr}\left(-2W^TX^TXW
+W^TX^TXWW^TW\right)\\
&amp;=\underset{W}{\min } \ \mathrm{Tr}\left(-W^TX^TXW\right)\\
&amp;=\underset{W}{\max } \ \mathrm{Tr}\left(W^TX^TXW\right)\\
&amp;=\underset{W}{\max } \
\mathrm{Tr}\left(\left(XW\right)^TXW\right)\\
&amp;=\underset{W}{\max } \|XW\|_{F}^{2}
\end{aligned}
\]</span></p>
<h2 id="求解">3. 求解</h2>
<p>由 <span class="math inline">\(\eqref{优化}\)</span> 可知，目标函数为
<span class="math display">\[\label{目标函数}
\begin{array}{c}
\underset{W}{\min }\ -\mathrm{Tr}\left(W^TX^TXW\right)=\underset{W}{\max
} \ \mathrm{Tr}\left(W^TX^TXW\right) \\
\text{s.t.}\quad W^TW=I_k
\end{array}
\]</span></p>
<p>可以用<strong>拉格朗日乘子法</strong>求解带约束的优化问题。</p>
<p>拉格朗日函数 <span class="math display">\[
\mathcal{L}\left(W,\Lambda\right)=-\mathrm{Tr}\left(W^TX^TXW\right)+\mathrm{Tr}\left(\Lambda^T
\left(W^TW-I_k\right)\right)
\]</span> 其中 <span class="math inline">\(\Lambda =
diag(\lambda_1,\lambda_2,\dots,\lambda_{k})\in \mathbb{R}^{k\times
k}\)</span></p>
<p>求解 <span class="math display">\[
\begin{aligned}
    \frac{\partial \mathcal{L}}{\partial W}&amp;= -2X^TXW +
2W\Lambda=0\\
    \frac{\partial \mathcal{L}}{\partial \Lambda}&amp;=W^TW-I_k=0 \\
\end{aligned}
\]</span></p>
<p>则有 <span class="math display">\[\label{显式解}
X^TXW=W\Lambda
\]</span></p>
<p>将 <span class="math inline">\(W,\Lambda\)</span> 展开 <span
class="math display">\[
\begin{aligned}\nonumber
W &amp;=
[\boldsymbol{w}_1,\boldsymbol{w}_2,\dots,\boldsymbol{w}_k]\in\mathbb{R}^{p\times
k} ,\quad \boldsymbol{w}_i\in\mathbb{R}^{p\times 1}\\
\Lambda &amp;= diag(\lambda_1,\lambda_2,\dots,\lambda_{k})\in
\mathbb{R}^{k\times k},\quad \lambda_i \in \mathbb{R}
\end{aligned}
\]</span> 注意到 <span class="math display">\[\label{约束}
\boldsymbol{w}_i^T\boldsymbol{w}_i=1 ,\quad
\boldsymbol{w}_i^T\boldsymbol{w}_j=0\ \left(i\neq j\right)
\]</span> 则有 <span class="math display">\[\label{特征解}
X^T X \boldsymbol{w}_i = \lambda_i \boldsymbol{w}_i \ ,\quad
i=1,2,\dots,k
\]</span> 显然，此式为矩阵特征值和特征向量的定义式，其中 <span
class="math inline">\(\lambda_i,w_i\)</span> 分别表示矩阵 <span
class="math inline">\(X^TX\)</span> 的特征值和单位特征向量。</p>
<p>因为 <span class="math inline">\(X^TX\)</span>
为实对称阵，而实对称阵不同特征值对应的特征向量之间是相互正交的，同一特征值的不同特征向量可以通过施密特正交化使其正交，所以通过
<span class="math inline">\(\eqref{特征解}\)</span> 求的 <span
class="math inline">\(\boldsymbol{w}_i\)</span> 满足约束 <span
class="math inline">\(\eqref{约束}\)</span>。<br />
根据拉格朗日乘子法的原理可知，此时求得的结果仅是最优解的必要条件，而且
<span class="math inline">\(X^TX\)</span> 有 <span
class="math inline">\(p\)</span>
个相互正交的单位特征向量，所以还需要从这 <span
class="math inline">\(p\)</span> 个特征向量里找出 <span
class="math inline">\(k\)</span>
个能使得目标函数达到最优值的特征向量作为最优解。</p>
<p>将 <span class="math inline">\(\eqref{显式解}\)</span> 带入目标函数
<span class="math inline">\(\eqref{目标函数}\)</span> <span
class="math display">\[
\begin{aligned}
\underset{W}{\max }\ \mathrm{Tr}\left(W^TX^TXW\right)
&amp;=\underset{W}{\max }\ \mathrm{Tr}\left(W^T W\Lambda\right)\\
&amp;=\underset{W}{\max }\ \mathrm{Tr}\left(\Lambda\right)\\
&amp;=\underset{W}{\max }\ \sum_{i=1}^{k}\lambda_i
\end{aligned}
\]</span> 显然此时只需要令 <span
class="math inline">\(\lambda_1,\lambda_2,\dots,\lambda_k\)</span> 和
<span
class="math inline">\(\boldsymbol{w}_1,\boldsymbol{w}_2,\dots,\boldsymbol{w}_k\)</span>
分别为矩阵 <span class="math inline">\(X^TX\)</span> 的前 <span
class="math inline">\(k\)</span>
大特征值及其对应的特征向量即可使目标函数达到最大值。</p>
<h2 id="算法流程">4. 算法流程</h2>
<p><span class="math display">\[
\begin{array}{r l}
\textbf{PCA 算法}\\
\hline
\textbf{输入：}
&amp;\text{1. 样本数据 } X\in\mathbb{R}^{n\times p},n \text{
是样本个数},p \text{ 是样本特征个数；}\\
&amp;\text{2. 降维维度 }k \\
\textbf{过程：}
&amp;\text{1. 数据预处理，所有样本去中心化；}\\
&amp;\text{2. 计算 }X^T X \text{，并对其特征分解；}\\
&amp;\text{3. 取前 } k \text{ 大个特征值所对应的特征向量 }
w_1,w_2,\dots,w_k \text{ ；}\\
\textbf{输出：}
&amp;\text{投影矩阵 } W=[w_1,w_2,\dots,w_k] \in \mathbb{R}^{p\times k}\\
\hline
\end{array}
\]</span></p>
<p>过程 2、3 也可以使用 SVD 分解，取前 <span
class="math inline">\(k\)</span> 个右奇异向量即可。</p>
<h2 id="方差贡献率">5. 方差贡献率</h2>
<p>第 <span class="math inline">\(i\)</span> 个主成分的方差贡献率 <span
class="math display">\[
\eta_i = \frac{\lambda_i}{\sum_{j=1}^{p}\lambda_j}
\]</span></p>
<p>前 <span class="math inline">\(k\)</span> 个主成分的方差贡献率 <span
class="math display">\[
\eta = \frac{\sum_{i=1}^{k}\lambda_i}{\sum_{j=1}^{p}\lambda_j}
\]</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>机器学习</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title>单像空间后方交会</title>
    <url>/2025/01/18/%E5%8D%95%E5%83%8F%E7%A9%BA%E9%97%B4%E5%90%8E%E6%96%B9%E4%BA%A4%E4%BC%9A/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="概要">1. 概要</h2>
<p>单像后方交会的基本思想是<strong>至少</strong>利用三个已知地面控制点的坐标
<span class="math inline">\(A(X_A,Y_A,Z_A)\)</span>，<span
class="math inline">\(B(X_B,Y_B,Z_B)\)</span>，<span
class="math inline">\(C(X_C,Y_C,Z_C)\)</span>
，与影像上对应的三个像点的影像坐标 <span
class="math inline">\(a(x_a,y_a)\)</span>，<span
class="math inline">\(b(x_b,y_b)\)</span>，<span
class="math inline">\(c(x_c,y_c)\)</span>
，再根据共线方程，反求该像片的外方位元素 <span
class="math inline">\(X_s,Y_s,Z_s,\varphi,\omega,\kappa\)</span>。这种解法是以单张像片为基础，所以也称做单像空间后方交会。</p>
<h2 id="推导">2. 推导</h2>
<h3 id="基本关系式">2.1. 基本关系式</h3>
<p>共线方程： <span class="math display">\[
\begin{array}{c}
x=-f\frac{a_1\left( X-X_s \right) +b_1\left( Y-Y_s \right) +c_1\left(
Z-Z_s \right)}{a_3\left( X-X_s \right) +b_3\left( Y-Y_s \right)
+c_3\left( Z-Z_s \right)} \\
y=-f\frac{a_2\left( X-X_s \right) +b_2\left( Y-Y_s \right) +c_2\left(
Z-Z_s \right)}{a_3\left( X-X_s \right) +b_3\left( Y-Y_s \right)
+c_3\left( Z-Z_s \right)} \\
\end{array}
\]</span></p>
<p>为了方便计算，需要把非线性的共线方程线性化。<span
class="math inline">\(x,y\)</span>
是关于六个外方位元素的多元函数，进行一次泰勒展开： <span
class="math display">\[\label{共线方程}
\begin{array}{c}
x = \tilde{x} + \frac{\partial x}{\partial X_s}dX_s + \frac{\partial
x}{\partial Y_s}dY_s+\frac{\partial x}{\partial Z_s}dZ_s+\frac{\partial
x}{\partial \varphi}d\varphi +\frac{\partial x}{\partial \omega}d\omega
+\frac{\partial x}{\partial \kappa}d\kappa  \\
y = \tilde{y} +\frac{\partial y}{\partial X_s}dX_s+\frac{\partial
y}{\partial Y_s}dY_s+\frac{\partial y}{\partial Z_s}dZ_s+\frac{\partial
y}{\partial \varphi}d\varphi +\frac{\partial y}{\partial \omega}d\omega
+\frac{\partial y}{\partial \kappa}d\kappa
\end{array}
\]</span> <span class="math inline">\(\tilde{x},\tilde{y}\)</span>
是函数的近似值，是把外方位元素的初值 <span
class="math inline">\(X_{s0},Y_{s0},Z_{s0},\varphi_0,\omega_0,\kappa_0\)</span>
带入共线方程中得到的结果；<br />
<span
class="math inline">\(dX_s,dY_x,dZ_s,d\varphi_,d\omega,d\kappa\)</span>
是对应外方位元素近似值的改正数（也就是增量）；<br />
<span class="math inline">\(\frac{\partial x}{\partial
X_s},\dots,\frac{\partial y}{\partial \kappa}\)</span>
为偏导数，是外方位元素改正数的系数。</p>
<p>为了方便表示，式 <span
class="math inline">\(\eqref{共线方程}\)</span> 记为： <span
class="math display">\[\label{共线方程简化}
\begin{array}{c}
x = \tilde{x} + a_{11}\ dX_s + a_{12}\ dY_s+a_{13}\ dZ_s+a_{14}\
d\varphi +a_{15}\ d\omega +a_{16}\ d\kappa= \tilde{x} + dx  \\
y = \tilde{y} +a_{21}\ dX_s+a_{22}\ dY_s+a_{23}\ dZ_s+a_{24}\ d\varphi
+a_{25}\ d\omega +a_{26}\ d\kappa=\tilde{y} + dy
\end{array}
\]</span> <span class="math inline">\(dx,dy\)</span>
表示近似值的改正值。</p>
<p>因为上述近似是一阶的，结果较为粗略，所以需要不断迭代趋近，直到改正值小于某一阈值。<br />
对于一个控制点就能由共线方程列写两个式子，所以一个像片中有三个点就能列出六个方程，求出六个外方位元素的改正值。</p>
<h3 id="误差方程与法方程">2.2. 误差方程与法方程</h3>
<p>为了提高解算外方位元素的精度，常有多余观测方程。在空间后方交会中，通常是在像片的四个角上选取四个或者更多的地面控制点，因此要用最小二乘法平差计算。<br />
<strong>计算中，通常将控制点的地面坐标视为真值，而把相应的像点坐标视为观测值</strong>，加入相应的改正数
<span class="math inline">\(v_x,v_y\)</span>，按照
<code>观测值 + 观测值改正数 = 近似值 + 近似值改正数</code>
的改正原则，得： <span class="math display">\[
\begin{array}{c}
x+v_x=\tilde{x}+dx \\
y+v_y=\tilde{y}+dy
\end{array}
\Longrightarrow
\begin{array}{c}
v_x=\tilde{x}+dx-x \\
v_y=\tilde{y}+dy-y
\end{array}
\]</span> 这里 <span class="math inline">\(v_x,v_y\)</span>
的意义就比较明显了，代表 近似值经过改正后与真实值之间的误差。</p>
<p>结合式 <span
class="math inline">\(\eqref{共线方程简化}\)</span>，我们可以用矩阵形式表示这个方程组
<span class="math display">\[
V=AX-l
\]</span> 其中 <span class="math display">\[
\begin{equation*}
\begin{aligned}
V&amp;=\left[v_x, v_y\right]^T \in\mathbb{R}^{2\times 1} \\\\
A&amp;=\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} &amp; a_{15} &amp; a_{16}
\\
a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} &amp; a_{25} &amp; a_{26}
\end{bmatrix} \in\mathbb{R}^{2\times 6} \\\\
X&amp;=\left[dX_s,dY_s,dZ_s,d\varphi,d\omega,d\kappa\right]^T
\in\mathbb{R}^{6\times 1} \\\\
l&amp;=\left[l_x,l_y\right]^T=\left[x-\tilde{x}, y-\tilde{y}\right]^T
\in\mathbb{R}^{2\times 1}
\end{aligned}
\end{equation*}
\]</span> 上面只是一个控制点的形式，当存在 <span
class="math inline">\(n\)</span> 个控制点的时候，则有： <span
class="math display">\[\label{误差方程}
V=AX-L
\]</span> 其中 <span class="math display">\[
\begin{equation*}
\begin{aligned}
V&amp;=\left[V_1^T , V_2^T,\dots,V_n^T\right]^T \in \mathbb{R}^{2n\times
1} \\\\
A&amp;=\left[A_1^T,A_2^T,\dots,A_n^T \right] \in\mathbb{R}^{2n\times 6}
\\\\
X&amp;=\left[dX_s,dY_s,dZ_s,d\varphi,d\omega,d\kappa\right]^T
\in\mathbb{R}^{6\times 1} \\\\
l&amp;=\left[l_1^T,l_2^T,\dots,l_n^T \right]^T \in\mathbb{R}^{2n\times
1}
\end{aligned}
\end{equation*}
\]</span> 注意矩阵的大小，写代码的时候要用到。</p>
<p>根据最小二乘间接平差原理，可列出法方程： <span
class="math display">\[
A^T P A X=A^T L
\]</span> 其中，<span class="math inline">\(P\)</span>
为像点观测值的权值矩阵，表示观测值测量的相对精度，一般认为是等精度测量，故取
<span class="math inline">\(P\)</span> 为单位阵，因此可解得： <span
class="math display">\[\label{封闭解}
X=(A^T A)^{-1}A^T L
\]</span> 从而求得外方位元素近似值的改正数 <span
class="math inline">\(dX_s,dY_s,dZ_s,d\varphi,d\omega,d\kappa\)</span>
。</p>
<details class="note info"><summary><p>一些解释</p>
</summary>
<p>这里的计算过程其实就是很常见的线性模型，最小化误差即 <span
class="math display">\[\nonumber X=arg\
\underset{X}{min}\|V^TV\|_{2}^{2}\]</span>
这是一个经典的凸优化问题，求偏导，令偏导等于 0 即可。</p>

</details>
<p>根据上面的内容，我们可以得到第 <span class="math inline">\(i\)</span>
次计算的外方位元素的改正值 <span
class="math inline">\(X_i\)</span>，直到第 <span
class="math inline">\(k\)</span> 次收敛的时，得到最后的外方位元素：
<span class="math display">\[
\begin{equation*}
\begin{aligned}
X_s &amp;= X_{s0}+dX_{s1}+dX_{s2}+\dots+dX_{sk} \\
Y_s &amp;= Y_{s0}+dY_{s1}+dY_{s2}+\dots+dY_{sk} \\
Z_s &amp;= Z_{s0}+dZ_{s1}+dZ_{s2}+\dots+dZ_{sk} \\
\varphi &amp;= \varphi_0 + d\varphi_1 + d\varphi_2+\dots+d\varphi_k \\
\omega &amp;= \omega_0 + d\omega_1+d\omega_2 + \dots+d\omega_k \\
\kappa &amp;= \kappa_0+d\kappa_1+d\kappa_2+\dots+d\kappa_k
\end{aligned}
\end{equation*}
\]</span></p>
<p>即 <span class="math display">\[
X = X_0 + X_1 + X_2 +\dots+X_k
\]</span></p>
<p>观察式 <span class="math inline">\(\eqref{封闭解}\)</span>
可知，目前仅 <span class="math inline">\(A\)</span> 未知，下面对 <span
class="math inline">\(A\)</span> 进行求解。 <details class="note info"><summary><p>求解 <span class="math inline">\(A\)</span></p>
</summary>
<p>为书写方便，令共线方程中的分子、分母如下： <span
class="math display">\[\begin{equation}\label{共线方程分子分母}
\begin{aligned}
\overline{X} = a_1\left( X-X_s \right) +b_1\left( Y-Y_s \right)
+c_1\left( Z-Z_s \right) \\
\overline{Y} = a_2\left( X-X_s \right) +b_2\left( Y-Y_s \right)
+c_2\left( Z-Z_s \right) \\
\overline{Z} = a_3\left( X-X_s \right) +b_3\left( Y-Y_s \right)
+c_3\left( Z-Z_s \right) \\
\end{aligned}
\end{equation}
\]</span></p>
<p>由 <span class="math inline">\(\eqref{共线方程}\)</span> 和 <span
class="math inline">\(\eqref{共线方程简化}\)</span> 可知 <span
class="math display">\[
\begin{equation*}
\begin{aligned}
a_{11} = \frac{\partial x}{\partial X_s} &amp;= \frac{\partial
\left(-f\frac{\overline{X}}{\overline{Z}}\right)}{\partial X_s} &amp;
分式求导\\
&amp;=-f \frac{\frac{\partial \overline{X}}{\partial X_s}\overline{Z} -
\overline{X}\frac{\partial \overline{Z}}{\partial X_s}}{\overline{Z}^2}
&amp;x=-f\frac{\overline{X}}{\overline{Z}}\\
&amp;=-f\left(-\frac{a_1}{\overline{Z}} - \frac{1}{f\overline{Z}}x
a_3\right) \\
&amp;=\frac{1}{\overline{Z}}\left(a_1 f + a_3 x\right)
\end{aligned}
\end{equation*}
\]</span></p>
<p>同理可得 <span class="math display">\[
\begin{align}
&amp;
\begin{cases}\label{对XYZ偏导}
a_{11}=\frac{\partial x}{\partial X_{s}}=\frac{1}{\overline{Z}}(a_{1}f +
a_{3}x)\\\\
a_{12}=\frac{\partial x}{\partial Y_{s}}=\frac{1}{\overline{Z}}(b_{1}f +
b_{3}x)\\\\
a_{13}=\frac{\partial x}{\partial Z_{s}}=\frac{1}{\overline{Z}}(c_{1}f +
c_{3}x)\\\\
a_{21}=\frac{\partial y}{\partial X_{s}}=\frac{1}{\overline{Z}}(a_{2}f +
a_{3}y)\\\\
a_{22}=\frac{\partial y}{\partial Y_{s}}=\frac{1}{\overline{Z}}(b_{2}f +
b_{3}y)\\\\
a_{23}=\frac{\partial y}{\partial Z_{s}}=\frac{1}{\overline{Z}}(c_{2}f +
c_{3}y)
\end{cases}
\\\nonumber \\
&amp;
\begin{cases}\label{对角度偏导}
a_{14}=\frac{\partial x}{\partial
\varphi}=-\frac{f}{(\overline{Z})^{2}}(\frac{\partial
\overline{X}}{\partial \varphi}\overline{Z}-\frac{\partial
\overline{Z}}{\partial \varphi}\overline{X})\\\\
a_{15}=\frac{\partial x}{\partial
\omega}=-\frac{f}{(\overline{Z})^{2}}(\frac{\partial
\overline{X}}{\partial \omega}\overline{Z}-\frac{\partial
\overline{Z}}{\partial \omega}\overline{X})\\\\
a_{16}=\frac{\partial x}{\partial
\kappa}=-\frac{f}{(\overline{Z})^{2}}(\frac{\partial
\overline{X}}{\partial \kappa}\overline{Z}-\frac{\partial
\overline{Z}}{\partial \kappa}\overline{X})\\\\
a_{24}=\frac{\partial y}{\partial
\varphi}=-\frac{f}{(\overline{Z})^{2}}(\frac{\partial
\overline{Y}}{\partial \varphi}\overline{Z}-\frac{\partial
\overline{Z}}{\partial \varphi}\overline{Y})\\\\
a_{25}=\frac{\partial y}{\partial
\omega}=-\frac{f}{(\overline{Z})^{2}}(\frac{\partial
\overline{Y}}{\partial \omega}\overline{Z}-\frac{\partial
\overline{Z}}{\partial \omega}\overline{Y})\\\\
a_{26}=\frac{\partial y}{\partial
\kappa}=-\frac{f}{(\overline{Z})^{2}}(\frac{\partial
\overline{Y}}{\partial \kappa}\overline{Z}-\frac{\partial
\overline{Z}}{\partial \kappa}\overline{Y})
\end{cases}
\end{align}
\]</span></p>
<p>由式 <span class="math inline">\(\eqref{共线方程分子分母}\)</span>
可知 <span class="math display">\[
\begin{aligned}
\left[ \begin{array}{c}
\overline{X} \\
\overline{Y} \\
\overline{Z} \\
\end{array} \right]
&amp;=\left[\begin{array}{c}
a_1 &amp; a_2 &amp; a_3 \\
b_1 &amp; b_2 &amp; b_3 \\
c_1 &amp; c_2 &amp; c_3 \\
\end{array}\right]
\left[\begin{array}{c}
X-X_s\\
Y-Y_s\\
Z-Z_s
\end{array}\right]
=R^T\left[\begin{array}{c}
X-X_s\\
Y-Y_s\\
Z-Z_s
\end{array}\right] \\
&amp;=R_{\kappa}^T R_{\omega}^T R_{\varphi}^T \left[\begin{array}{c}
X-X_s\\
Y-Y_s\\
Z-Z_s
\end{array}\right]
=R_{\kappa}^{-1} R_{\omega}^{-1} R_{\varphi}^{-1} \left[\begin{array}{c}
X-X_s\\
Y-Y_s\\
Z-Z_s
\end{array}\right]
\end{aligned}
\]</span></p>
<p>所以 <span class="math display">\[
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \varphi}
\begin{bmatrix}
\overline{X} \\
\overline{Y} \\
\overline{Z}
\end{bmatrix}
&amp;= R_{\kappa}^{-1} R_{\omega}^{-1} \frac{\partial
R_{\varphi}^{-1}}{\partial \varphi}
\begin{bmatrix}
X - X_{s} \\
Y - Y_{s} \\
Z - Z_{s}
\end{bmatrix}\\
&amp;= R_{\kappa}^{-1} R_{\omega}^{-1} R_{\varphi}^{-1} R_{\varphi}
\frac{\partial R_{\varphi}^{-1}}{\partial \varphi}
\begin{bmatrix}
X - X_{s} \\
Y - Y_{s} \\
Z - Z_{s}
\end{bmatrix}\\
&amp;= R^{-1} R_{\varphi} \frac{\partial R_{\varphi}^{-1}}{\partial
\varphi}
\begin{bmatrix}
X - X_{s} \\
Y - Y_{s} \\
Z - Z_{s}
\end{bmatrix}
\end{aligned}
\end{equation}
\]</span></p>
<p>而 <span class="math display">\[
R_{\varphi}^{-1} = R_{\varphi}^{\mathrm{T}}
=\begin{bmatrix}
\cos\varphi &amp; 0 &amp; \sin\varphi \\
0 &amp; 1 &amp; 0 \\
-\sin\varphi &amp; 0 &amp; \cos\varphi
\end{bmatrix}
\]</span> 则 <span class="math display">\[
R_{\varphi} \frac{\partial R_{\varphi}^{-1}}{\partial \varphi}
=\begin{bmatrix}
\cos\varphi &amp; 0 &amp; -\sin\varphi \\
0 &amp; 1 &amp; 0 \\
\sin\varphi &amp; 0 &amp; \cos\varphi
\end{bmatrix}
\begin{bmatrix}
-\sin\varphi &amp; 0 &amp; \cos\varphi \\
0 &amp; 0 &amp; 0 \\
-\cos\varphi &amp; 0 &amp; -\sin\varphi
\end{bmatrix}
=\begin{bmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; 0
\end{bmatrix}
\]</span></p>
<p>代入上式，得： <span class="math display">\[
\begin{equation}\label{varphi偏导}
\begin{aligned}
\frac{\partial}{\partial \varphi}
\begin{bmatrix}
\overline{X} \\
\overline{Y} \\
\overline{Z}
\end{bmatrix}
&amp;=\begin{bmatrix}
a_{1} &amp; b_{1} &amp; c_{1} \\
a_{2} &amp; b_{2} &amp; c_{2} \\
a_{3} &amp; b_{3} &amp; c_{3}
\end{bmatrix}
\begin{bmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
X - X_{s} \\
Y - Y_{s} \\
Z - Z_{s}
\end{bmatrix}\\
&amp;=\begin{bmatrix}
a_{1} &amp; b_{1} &amp; c_{1} \\
a_{2} &amp; b_{2} &amp; c_{2} \\
a_{3} &amp; b_{3} &amp; c_{3}
\end{bmatrix}
\begin{bmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
a_{1} &amp; a_{2} &amp; a_{3} \\
b_{1} &amp; b_{2} &amp; b_{3} \\
c_{1} &amp; c_{2} &amp; c_{3}
\end{bmatrix}
\begin{bmatrix}
\overline{X} \\
\overline{Y} \\
\overline{Z}
\end{bmatrix}\\
&amp;=\begin{bmatrix}
0 &amp; -b_{3} &amp; b_{2} \\
b_{3} &amp; 0 &amp; -b_{1} \\
-b_{2} &amp; b_{1} &amp; 0
\end{bmatrix}
\begin{bmatrix}
\overline{X} \\
\overline{Y} \\
\overline{Z}
\end{bmatrix}
\end{aligned}
\end{equation}
\]</span> 按相仿的方法，得： <span class="math display">\[
\begin{equation}\label{omega偏导}
\begin{aligned}
    \frac{\partial}{\partial \omega}
    \begin{bmatrix}
    \overline{X} \\
    \overline{Y} \\
    \overline{Z}
    \end{bmatrix}
    &amp;= R_{\kappa}^{-1} \frac{\partial R_{\omega}^{-1}}{\partial
\omega} R_{\varphi}^{-1}
    \begin{bmatrix}
    X - X_{s} \\
    Y - Y_{s} \\
    Z - Z_{s}
    \end{bmatrix}\\
    &amp;= R_{\kappa}^{-1} \frac{\partial R_{\omega}^{-1}}{\partial
\omega} R_{\omega} R_{\kappa} R_{\kappa}^{-1} R_{\omega}^{-1}
R_{\varphi}^{-1}
    \begin{bmatrix}
    X - X_{s} \\
    Y - Y_{s} \\
    Z - Z_{s}
    \end{bmatrix}\\
    &amp;=R_{\kappa}^{-1}
    \begin{bmatrix}
    0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 \\
    0 &amp; -1 &amp; 0
    \end{bmatrix}
    R_{\kappa}R^{-1}
    \begin{bmatrix}
    X - X_{s} \\
    Y - Y_{s} \\
    Z - Z_{s}
    \end{bmatrix}\\
    &amp;=\begin{bmatrix}
    \overline{Z}\sin\kappa \\
    \overline{Z}\cos\kappa \\
    -\overline{X}\sin\kappa - Y\cos\kappa
    \end{bmatrix}
\end{aligned}
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}\label{kappa偏导}
\begin{aligned}
    \frac{\partial}{\partial \kappa}
    \begin{bmatrix}
    \overline{X} \\
    \overline{Y} \\
    \overline{Z}
    \end{bmatrix}
    &amp;=\frac{\partial R_{\kappa}^{-1}}{\partial
\kappa}R_{\kappa}R_{\kappa}^{-1}R_{\omega}^{-1}R_{\varphi}^{-1}
    \begin{bmatrix}
    X - X_{s} \\
    Y - Y_{s} \\
    Z - Z_{s}
    \end{bmatrix}\\
    &amp;=\begin{bmatrix}
    0 &amp; 1 &amp; 0 \\
    -1 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0
    \end{bmatrix}
    \begin{bmatrix}
    a_{1} &amp; b_{1} &amp; c_{1} \\
    a_{2} &amp; b_{2} &amp; c_{2} \\
    a_{3} &amp; b_{3} &amp; c_{3}
    \end{bmatrix}
    \begin{bmatrix}
    X - X_{s} \\
    Y - Y_{s} \\
    Z - Z_{s}
    \end{bmatrix}\\
    &amp;=\begin{bmatrix}
    \overline{Y} \\
    -\overline{X} \\
    0
    \end{bmatrix}
\end{aligned}
\end{equation}
\]</span></p>
<p>将 <span class="math inline">\(\eqref{varphi偏导} \eqref{omega偏导}
\eqref{kappa偏导}\)</span> 带入 <span
class="math inline">\(\eqref{对角度偏导}\)</span>，整理得： <span
class="math display">\[
\begin{equation}\label{对角度偏导化简}
\begin{aligned}
a_{14} &amp;= y\sin\omega - \left[\frac{x}{f}(x\cos\kappa - y\sin\kappa)
+ f\cos\kappa\right]\cos\omega\\
a_{15} &amp;= -f\sin\kappa - \frac{x}{f}(x\sin\kappa + y\cos\kappa)\\
a_{16} &amp;= y\\
a_{24} &amp;= -x\sin\omega - \left[\frac{y}{f}(x\cos\kappa -
y\sin\kappa) - f\sin\kappa\right]\cos\omega\\
a_{25} &amp;= -f\cos\kappa - \frac{y}{f}(x\sin\kappa + y\cos\kappa)\\
a_{26} &amp;= -x
\end{aligned}
\end{equation}
\]</span></p>

</details></p>
<p>在<strong>竖直摄影</strong>情况下，角元素都是小角<span
class="math inline">\((\lt 3°)\)</span>，可用 <span
class="math inline">\(\varphi=\omega=\kappa=0\)</span> 及 <span
class="math inline">\(Z-Z_s=-H\)</span> 代替，得到各系数的近似值： <span
class="math display">\[
\begin{equation}
\begin{aligned}
a_{11} &amp;= -\frac{f}{H} &amp; a_{12} &amp;= 0 &amp; a_{13} &amp;=
-\frac{x}{H}\\
a_{14} &amp;= -f\left(1 + \frac{x^2}{f^2}\right) &amp; a_{15} &amp;=
-\frac{xy}{f} &amp; a_{16} &amp;= y\\
a_{21} &amp;= 0 &amp; a_{22} &amp;= -\frac{f}{H} &amp; a_{23} &amp;=
-\frac{y}{H}\\
a_{24} &amp;= -\frac{xy}{f} &amp; a_{25} &amp;= -f\left(1 +
\frac{y^2}{f^2}\right) &amp; a_{26} &amp;= -x
\end{aligned}
\end{equation}
\]</span></p>
<h2 id="解算步骤">3. 解算步骤</h2>
<p>综上所述，空间后方交会的求解过程如下：</p>
<ol type="1">
<li>获取已知数据：从摄影资料中查取像片比例尺<span
class="math inline">\(1/m\)</span>，平均航高，内方元素<span
class="math inline">\(x_0\)</span>、<span
class="math inline">\(y_0\)</span>、<span
class="math inline">\(f\)</span>；从外业测量成果中，获取控制点的地面测量坐标<span
class="math inline">\(X_t\)</span>、<span
class="math inline">\(Y_t\)</span>、<span
class="math inline">\(Z_t\)</span>，并转化成地面摄影测量坐标<span
class="math inline">\(X\)</span>、<span
class="math inline">\(Y\)</span>、<span
class="math inline">\(Z\)</span>。</li>
<li>量测控制点的像点坐标：将控制点标刺在像片上，利用立体坐标量测仪量测控制点的像框标坐标，并经像点坐标改正，得到像点坐标<span
class="math inline">\(x\)</span>、<span
class="math inline">\(y\)</span>。</li>
<li>确定未知数的初始值：在竖直摄影情况下，角元素的初始值为<span
class="math inline">\(0\)</span>，即<span
class="math inline">\(\varphi_0 = \omega_0 = \kappa_0 =
0\)</span>；线元素中，<span class="math inline">\(Z_{s0} = H =
mf\)</span>，<span class="math inline">\(X_{s0}\)</span>、<span
class="math inline">\(Y_{s0}\)</span>的取值可用四个角上控制点坐标的平均值，即：<span
class="math inline">\(X_{s0} = \frac{1}{4} \sum_{i = 1}^{4}
X_i\)</span>，<span class="math inline">\(Y_{s0} = \frac{1}{4} \sum_{i =
1}^{4} Y_i\)</span>。</li>
<li>计算旋转矩阵<span
class="math inline">\(R\)</span>：利用角元素的近似值计算方向余弦值，组成<span
class="math inline">\(R\)</span>阵。</li>
<li>逐点计算像点坐标的近似值：利用未知数的近似值按共线方程式计算控制点像点坐标的近似值<span
class="math inline">\(\tilde{x}\)</span>、<span
class="math inline">\(\tilde{y}\)</span>。</li>
<li>组成误差方程式：按式 <span
class="math inline">\(\eqref{对XYZ偏导}\eqref{对角度偏导化简}\eqref{误差方程}\)</span>
逐点计算误差方程式的系数和常数项。</li>
<li>组成法方程式：计算法方程的系数矩阵<span
class="math inline">\(A^{\mathrm{T}}A\)</span>与常数项<span
class="math inline">\(A^{\mathrm{T}}L\)</span>。</li>
<li>解求外方位元素：根据法方程，按式<span
class="math inline">\(\eqref{封闭解}\)</span>解求外方位元素改正数，并与相应的近似值求和，得到外方位元素新的近似值。</li>
<li>检查计算是否收敛：将求得的外方位元素的改正数与规定的限差比较，小于限差则计算终止，否则用新的近似值重复第4至第8步骤的计算，直到满足要求为止。</li>
</ol>
<h2 id="精度">4. 精度</h2>
<p>由平差理论可知，法方程系数的逆矩阵 <span
class="math inline">\((A^{\mathrm{T}}A)^{-1}\)</span>
等于未知数的协因数阵 <span
class="math inline">\(Q_{x}\)</span>，因此可按下式计算未知数的中误差：
<span class="math display">\[
m_{i}=m_{0} \cdot \sqrt{Q_{ii}}
\]</span> 式中，<span class="math inline">\(i\)</span>
表示相应的未知数，<span class="math inline">\(Q_{ii}\)</span> 为 <span
class="math inline">\(Q_{x}\)</span> 阵中的主对角线元素，<span
class="math inline">\(m_{0}\)</span> 称为单位权中误差，计算公式为：
<span class="math display">\[
m_{0}=\pm\sqrt{\frac{\|V^TV\|_{2}^{2}}{2n - 6}}
\]</span> 这里，<span class="math inline">\(n\)</span>
表示控制点的总数。</p>
<h2 id="python-实现">5. python 实现</h2>
<p>以 <a href="#reference1">[1]</a> 的课后题为例</p>
<p>目录结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├─mian.py</span><br><span class="line">├─data.txt</span><br><span class="line">├─result.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>main.py</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> cos, sin</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">path</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        raw_data = f.read()</span><br><span class="line">        <span class="comment"># print(raw_data)</span></span><br><span class="line">    raw_data = raw_data.split(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(raw_data)):</span><br><span class="line">        tmp = [np.float64(it) <span class="keyword">for</span> it <span class="keyword">in</span> raw_data[i].split()]</span><br><span class="line">        data.extend([tmp])</span><br><span class="line">    <span class="keyword">return</span> np.array(data)[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_path = <span class="string">&quot;data.txt&quot;</span></span><br><span class="line">save_path = <span class="string">&quot;result.txt&quot;</span></span><br><span class="line"></span><br><span class="line">m = <span class="number">4_0000</span>  <span class="comment"># 比例尺 1/m</span></span><br><span class="line">f = <span class="number">0.15324</span>  <span class="comment"># 主距</span></span><br><span class="line">x0, y0 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">phi, omega, kappa = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>  <span class="comment"># 俯仰角、横摇角和偏航角</span></span><br><span class="line"></span><br><span class="line">data = get_data(data_path)  <span class="comment"># 像坐标和地面坐标数据</span></span><br><span class="line">n_points = data.shape[<span class="number">0</span>]  <span class="comment"># 控制点的个数</span></span><br><span class="line">img_coord = data[:, :<span class="number">2</span>] / <span class="number">1000</span>  <span class="comment"># 像点坐标，除以 1000 将单位换算为 m</span></span><br><span class="line">ground_coord = data[:, <span class="number">2</span>:]  <span class="comment"># 地面坐标</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Xs = np.mean(ground_coord[:, <span class="number">0</span>])</span><br><span class="line">Ys = np.mean(ground_coord[:, <span class="number">1</span>])</span><br><span class="line">Zs = m * f</span><br><span class="line"><span class="comment"># print(f&quot;外方位元素初始值：&#123;Xs,Ys,Zs,phi,omega,kappa&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line">R = np.zeros((<span class="number">3</span>, <span class="number">3</span>))  <span class="comment"># 旋转矩阵</span></span><br><span class="line">x = y = np.zeros((n_points, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># V = AX - L</span></span><br><span class="line">V = L = np.zeros((<span class="number">2</span> * n_points, <span class="number">1</span>))</span><br><span class="line">A = np.zeros((<span class="number">2</span> * n_points, <span class="number">6</span>))</span><br><span class="line">X = np.zeros((<span class="number">6</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">n_iter, MAX_ITER = <span class="number">0</span>, <span class="number">150</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    a1 = R[<span class="number">0</span>, <span class="number">0</span>] = cos(phi) * cos(kappa) - sin(phi) * sin(omega) * sin(kappa)</span><br><span class="line">    a2 = R[<span class="number">0</span>, <span class="number">1</span>] = (-<span class="number">1.0</span>) * cos(phi) * sin(kappa) - sin(phi) * sin(omega) * cos(kappa)</span><br><span class="line">    a3 = R[<span class="number">0</span>, <span class="number">2</span>] = (-<span class="number">1.0</span>) * sin(phi) * cos(omega)</span><br><span class="line">    b1 = R[<span class="number">1</span>, <span class="number">0</span>] = cos(omega) * sin(kappa)</span><br><span class="line">    b2 = R[<span class="number">1</span>, <span class="number">1</span>] = cos(omega) * cos(kappa)</span><br><span class="line">    b3 = R[<span class="number">1</span>, <span class="number">2</span>] = (-<span class="number">1.0</span>) * sin(omega)</span><br><span class="line">    c1 = R[<span class="number">2</span>, <span class="number">0</span>] = sin(phi) * cos(kappa) + cos(phi) * sin(omega) * sin(kappa)</span><br><span class="line">    c2 = R[<span class="number">2</span>, <span class="number">1</span>] = (-<span class="number">1.0</span>) * sin(phi) * sin(kappa) + cos(phi) * sin(omega) * cos(kappa)</span><br><span class="line">    c3 = R[<span class="number">2</span>, <span class="number">2</span>] = cos(phi) * cos(omega)</span><br><span class="line"></span><br><span class="line">    X_bar = a1 * (ground_coord[:, <span class="number">0</span>] - Xs) + b1 * (ground_coord[:, <span class="number">1</span>] - Ys) + c1 * (ground_coord[:, <span class="number">2</span>] - Zs)</span><br><span class="line">    Y_bar = a2 * (ground_coord[:, <span class="number">0</span>] - Xs) + b2 * (ground_coord[:, <span class="number">1</span>] - Ys) + c2 * (ground_coord[:, <span class="number">2</span>] - Zs)</span><br><span class="line">    Z_bar = a3 * (ground_coord[:, <span class="number">0</span>] - Xs) + b3 * (ground_coord[:, <span class="number">1</span>] - Ys) + c3 * (ground_coord[:, <span class="number">2</span>] - Zs)</span><br><span class="line"></span><br><span class="line">    x = (-<span class="number">1.0</span>) * f * X_bar / Z_bar</span><br><span class="line">    y = (-<span class="number">1.0</span>) * f * Y_bar / Z_bar</span><br><span class="line"></span><br><span class="line">    L = (img_coord - np.array((x, y)).T).flatten()</span><br><span class="line">    H = Zs - ground_coord[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    A[::<span class="number">2</span>, <span class="number">0</span>] = (-<span class="number">1.0</span>) * f / H</span><br><span class="line">    A[::<span class="number">2</span>, <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    A[::<span class="number">2</span>, <span class="number">2</span>] = (-<span class="number">1.0</span>) * x / H</span><br><span class="line">    A[::<span class="number">2</span>, <span class="number">3</span>] = (-<span class="number">1.0</span>) * f * (<span class="number">1</span> + x**<span class="number">2</span> / f**<span class="number">2</span>)</span><br><span class="line">    A[::<span class="number">2</span>, <span class="number">4</span>] = (-<span class="number">1.0</span>) * x * y / f</span><br><span class="line">    A[::<span class="number">2</span>, <span class="number">5</span>] = y</span><br><span class="line"></span><br><span class="line">    A[<span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    A[<span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>] = (-<span class="number">1.0</span>) * f / H</span><br><span class="line">    A[<span class="number">1</span>::<span class="number">2</span>, <span class="number">2</span>] = (-<span class="number">1.0</span>) * y / H</span><br><span class="line">    A[<span class="number">1</span>::<span class="number">2</span>, <span class="number">3</span>] = (-<span class="number">1.0</span>) * x * y / f</span><br><span class="line">    A[<span class="number">1</span>::<span class="number">2</span>, <span class="number">4</span>] = (-<span class="number">1.0</span>) * f * (<span class="number">1</span> + y**<span class="number">2</span> / f**<span class="number">2</span>)</span><br><span class="line">    A[<span class="number">1</span>::<span class="number">2</span>, <span class="number">5</span>] = (-<span class="number">1.0</span>) * x</span><br><span class="line"></span><br><span class="line">    ATA_inv = np.linalg.inv(A.T @ A)</span><br><span class="line">    X = ATA_inv @ A.T @ L</span><br><span class="line"></span><br><span class="line">    Xs += X[<span class="number">0</span>]</span><br><span class="line">    Ys += X[<span class="number">1</span>]</span><br><span class="line">    Zs += X[<span class="number">2</span>]</span><br><span class="line">    phi += X[<span class="number">3</span>]</span><br><span class="line">    omega += X[<span class="number">4</span>]</span><br><span class="line">    kappa += X[<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">    n_iter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (X[<span class="number">3</span>] &lt; <span class="number">1e-6</span> <span class="keyword">and</span> X[<span class="number">4</span>] &lt; <span class="number">1e-6</span> <span class="keyword">and</span> X[<span class="number">5</span>] &lt; <span class="number">1e-6</span>) <span class="keyword">or</span> (n_iter &gt; MAX_ITER):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(&quot;迭代次数:%0.f\n&quot; % n_iter)</span></span><br><span class="line"></span><br><span class="line">R[<span class="number">0</span>, <span class="number">0</span>] = cos(phi) * cos(kappa) - sin(phi) * sin(omega) * sin(kappa)</span><br><span class="line">R[<span class="number">0</span>, <span class="number">1</span>] = (-<span class="number">1.0</span>) * cos(phi) * sin(kappa) - sin(phi) * sin(omega) * cos(kappa)</span><br><span class="line">R[<span class="number">0</span>, <span class="number">2</span>] = (-<span class="number">1.0</span>) * sin(phi) * cos(omega)</span><br><span class="line">R[<span class="number">1</span>, <span class="number">0</span>] = cos(omega) * sin(kappa)</span><br><span class="line">R[<span class="number">1</span>, <span class="number">1</span>] = cos(omega) * cos(kappa)</span><br><span class="line">R[<span class="number">1</span>, <span class="number">2</span>] = (-<span class="number">1.0</span>) * sin(omega)</span><br><span class="line">R[<span class="number">2</span>, <span class="number">0</span>] = sin(phi) * cos(kappa) + cos(phi) * sin(omega) * sin(kappa)</span><br><span class="line">R[<span class="number">2</span>, <span class="number">1</span>] = (-<span class="number">1.0</span>) * sin(phi) * sin(kappa) + cos(phi) * sin(omega) * cos(kappa)</span><br><span class="line">R[<span class="number">2</span>, <span class="number">2</span>] = cos(phi) * cos(omega)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度计算</span></span><br><span class="line">V = A @ X - L</span><br><span class="line">VV = np.<span class="built_in">sum</span>(np.linalg.norm(V, <span class="built_in">ord</span>=<span class="number">2</span>) ** <span class="number">2</span>)</span><br><span class="line">m0 = np.sqrt(VV / (<span class="number">2</span> * n_points - <span class="number">6</span>))</span><br><span class="line">Qx = ATA_inv.diagonal()</span><br><span class="line">m = m0 * np.sqrt(Qx)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(save_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.writelines(<span class="string">&quot;外方位元素及其精度： \n&quot;</span>)</span><br><span class="line">    f.writelines(<span class="string">f&quot;Xs:<span class="subst">&#123;Xs:<span class="number">.6</span>f&#125;</span>  m:±<span class="subst">&#123;m[<span class="number">0</span>]:<span class="number">.6</span>f&#125;</span>\n&quot;</span>)</span><br><span class="line">    f.writelines(<span class="string">f&quot;Ys:<span class="subst">&#123;Ys:<span class="number">.6</span>f&#125;</span>  m:±<span class="subst">&#123;m[<span class="number">1</span>]:<span class="number">.6</span>f&#125;</span>\n&quot;</span>)</span><br><span class="line">    f.writelines(<span class="string">f&quot;Zs:<span class="subst">&#123;Zs:<span class="number">.6</span>f&#125;</span>  m:±<span class="subst">&#123;m[<span class="number">2</span>]:<span class="number">.6</span>f&#125;</span>\n&quot;</span>)</span><br><span class="line">    f.writelines(<span class="string">f&quot;phi:<span class="subst">&#123;phi:<span class="number">.6</span>f&#125;</span>  m:±<span class="subst">&#123;m[<span class="number">3</span>]:<span class="number">.6</span>f&#125;</span>\n&quot;</span>)</span><br><span class="line">    f.writelines(<span class="string">f&quot;omega:<span class="subst">&#123;omega:<span class="number">.6</span>f&#125;</span>  m:±<span class="subst">&#123;m[<span class="number">4</span>]:<span class="number">.6</span>f&#125;</span>\n&quot;</span>)</span><br><span class="line">    f.writelines(<span class="string">f&quot;kappa:<span class="subst">&#123;kappa:<span class="number">.6</span>f&#125;</span>  m:±<span class="subst">&#123;m[<span class="number">5</span>]:<span class="number">.6</span>f&#125;</span>\n&quot;</span>)</span><br><span class="line">    f.writelines(<span class="string">&quot;\n旋转矩阵：\n&quot;</span>)</span><br><span class="line">    f.write(<span class="string">f&quot;<span class="subst">&#123;R&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><figcaption><span>data.txt</span></figcaption><table><tr><td class="code"><pre><span class="line">NO.            x(mm)          y(mm)          X(m)           Y(m)           Z(m)</span><br><span class="line">1              -86.15         -68.99         36589.41       25273.32       2195.17</span><br><span class="line">2              -53.40         82.21          37631.08       31324.51       728.69         </span><br><span class="line">3              -14.78         -76.63         39100.97       24934.98       2386.50</span><br><span class="line">4              10.46          64.43          40426.54       30319.81       757.31</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><figcaption><span>result.txt</span></figcaption><table><tr><td class="code"><pre><span class="line">外方位元素及其精度： </span><br><span class="line">Xs:39795.443401  m:±1.125402</span><br><span class="line">Ys:27476.464840  m:±1.243674</span><br><span class="line">Zs:7572.688331  m:±0.483771</span><br><span class="line">phi:-0.003986  m:±0.000182</span><br><span class="line">omega:0.002114  m:±0.000160</span><br><span class="line">kappa:-0.067578  m:±0.000072</span><br><span class="line"></span><br><span class="line">旋转矩阵：</span><br><span class="line">[[ 0.99770901  0.06753409  0.00398554]</span><br><span class="line"> [-0.06752607  0.99771527 -0.00211367]</span><br><span class="line"> [-0.00411918  0.0018397   0.99998982]]</span><br></pre></td></tr></table></figure>
<p>如果你觉得这篇博客对你有所帮助，不妨在 GitHub 上给它点个
star🌟，这将是对我莫大的鼓励。<br />
<a
href="https://github.com/Blackspace2/space-resection-of-single-image.git">Github
地址</a></p>
<h2 id="参考资料">6. 参考资料</h2>
<p><strong>[1]</strong>
王佩军，徐亚明等编著.摄影测量学[M].武汉：武汉大学出版社，2016.5 <span
id="reference1"></span></p>
]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>后方交会</tag>
        <tag>摄影测量</tag>
      </tags>
  </entry>
  <entry>
    <title>常用矩阵计算</title>
    <url>/2025/01/15/%E5%B8%B8%E7%94%A8%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="符号约定">1. 符号约定</h2>
<p>正常小写字母表示标量；<br />
加粗小写字母表示向量，无特殊说明时通常为<strong>列向量</strong>；<br />
大写字母表示矩阵；</p>
<h2 id="求导">2. 求导</h2>
<p>矩阵求导中的 <strong>分子布局</strong> 和 <strong>分母布局</strong>
是两种不同的约定方式，它们的主要区别在于约定导数的结果的维度和排列方式，二者均正确，但需要保障在同一环境下只能使用一种布局方式。<br />
区分是分子布局还是分母布局，可以看求导结果的第一个维度是等于分子的第一个维度还是分母的第一个维度。<br />
可以注意到分子布局和分母布局的结果互为转置。</p>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 36%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">分母布局</th>
<th style="text-align: center;">分子布局</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial A\boldsymbol{x}}{\partial \boldsymbol{x}}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
A^T\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
A\]</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial \boldsymbol{\alpha}^{T}\boldsymbol{x}}{\partial
\boldsymbol{x}} = \frac{\partial
\boldsymbol{x}^{T}\boldsymbol{\alpha}}{\partial
\boldsymbol{x}}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
\boldsymbol{\alpha}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
\boldsymbol{\alpha}^T\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial\boldsymbol{y}^{T}\boldsymbol{Ax}}{\partial
A}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
\boldsymbol{y}\boldsymbol{x}^{T}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
\boldsymbol{y}^{T}\boldsymbol{x}\]</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial
\boldsymbol{x}^TA\boldsymbol{x}}{\partial\boldsymbol{x}}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
\boldsymbol{Ax}+A^T\boldsymbol{x}\,,\ 2\boldsymbol{Ax}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
\boldsymbol{x}^TA^T+\boldsymbol{x}^TA\,,\ 2\boldsymbol{x}^T
A\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial\  \mathrm{Tr}\left(X^TAX\right)}{\partial
X}=\frac{\partial\  \mathrm{Tr}\left(AXX^T\right)}{\partial
X}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
(A+A^T)X\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
X^T(A+A^T)\]</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial\  \mathrm{Tr}\left(XAX^T\right)}{\partial
X}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
X(A^T+A)\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
(A^T+A)X^T\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial \ \mathrm{Tr}(AX)}{\partial X} = \frac{\partial \
\mathrm{Tr}(XA)}{\partial X}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
A^T\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
A\]</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math display">\[\nonumber
\frac{\partial \ \mathrm{Tr}(AX^T)}{\partial X} = \frac{\partial \
\mathrm{Tr}(X^TA)}{\partial X}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
A\]</span></td>
<td style="text-align: center;"><span class="math display">\[\nonumber
A^T\]</span></td>
</tr>
</tbody>
</table>
<h2 id="雅可比-jacobi-矩阵">3. 雅可比 (Jacobi) 矩阵</h2>
<h2 id="黑塞-hessian-矩阵">4. 黑塞 (Hessian) 矩阵</h2>
]]></content>
      <categories>
        <category>数学</category>
        <category>矩阵计算</category>
      </categories>
      <tags>
        <tag>矩阵计算</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机-SVM</title>
    <url>/2025/01/18/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="概要">1. 概要</h2>
<p>支持向量机 (support vector machine, SVM)</p>
<ul>
<li>有监督学习</li>
<li><strong>线性</strong>分类器</li>
<li>利用<strong>核技巧</strong>解决<strong>非线性</strong>分类问题</li>
</ul>
<p><strong>为方便可视化，以二维二分类问题为例子对 SVM
进行说明。</strong></p>
<strong>目标</strong>：<span
id="SVM_purpose"></span>支持向量机想要求解的是离正负样本都尽可能远且刚好位于“正中间”的划分超平面，因为这样的超平面理论上泛化性能更好。<br />
<span id="fig1"></span>
<figure>
<img src="SVM示意图.svg" alt="SVM示意图" width="50%" height="50%" loading="lazy"/>
<figcaption>
图1. SVM示意图，蓝色为负例，红色为正例，<span
class="math inline">\(H_0\)</span> 为所求
</figcaption>
</figure>
<h2 id="符号说明与一些定义">2. 符号说明与一些定义</h2>
<h3 id="符号说明">2.1. 符号说明</h3>
<ul>
<li>数据集 <span
class="math inline">\(T=\{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}\)</span>
<ul>
<li><span class="math inline">\(\boldsymbol{x}_i \in \mathbb{R}^d,\quad
y_i \in \{+1,-1\},\quad i=1,2,\dots,n\)</span></li>
<li><span class="math inline">\(y_i=1\)</span> 时 <span
class="math inline">\(\boldsymbol{x}_i\)</span> 为正例，<span
class="math inline">\(y_i=-1\)</span> 时 <span
class="math inline">\(\boldsymbol{x}_i\)</span> 为负例</li>
<li><strong>假设数据是线性可分的</strong>，即存在一个超平面能够将不同类的样本完全分开。</li>
</ul></li>
<li>超平面 <span
class="math inline">\(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}+b=0\)</span>，简记为<span
class="math inline">\((\boldsymbol{w},b)\)</span>
<ul>
<li><strong>缩放不变性</strong>：<span
class="math inline">\(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}+b=0\)</span>
与 <span
class="math inline">\((k\boldsymbol{w})^\mathrm{T}\boldsymbol{x}+kb=0\
(k\gt 0)\)</span> 表示的是同一个超平面</li>
</ul></li>
</ul>
<details class="note info"><summary><p>关于<strong>缩放不变性</strong></p>
</summary>
<p>虽然 <span class="math inline">\(k&lt;0\)</span>
在数学上也满足缩放不变性，但在 SVM 的推导和实现中，通常限制 <span
class="math inline">\(k&gt;0\)</span>
以避免分类方向的反转和优化问题的复杂性。下面是一个简单的例子：</p>
<p>假设原始超平面参数为 <span class="math inline">\(\boldsymbol{w} = (1,
0)\)</span>，<span class="math inline">\(b =
0\)</span>，分类规则为：<br />
如果 <span class="math inline">\(x_1 &gt; 0\)</span>，则预测为 <span
class="math inline">\(y = +1\)</span>；<br />
如果 <span class="math inline">\(x_1 &lt; 0\)</span>，则预测为 <span
class="math inline">\(y = -1\)</span>。</p>
<p><strong>情况 1：<span class="math inline">\(k &gt; 0\)</span>（例如
<span class="math inline">\(k = 2\)</span>）</strong><br />
缩放后的超平面参数为 <span class="math inline">\(\boldsymbol{w}&#39; =
(2, 0)\)</span>，<span class="math inline">\(b&#39; =
0\)</span>，分类规则不变：<br />
如果 <span class="math inline">\(2x_1 &gt; 0 \quad \Rightarrow \quad x_1
&gt; 0\)</span>，则预测为 <span class="math inline">\(y =
+1\)</span>;<br />
如果 <span class="math inline">\(2x_1 &lt; 0 \quad \Rightarrow \quad x_1
&lt; 0\)</span>，则预测为 <span class="math inline">\(y =
-1\)</span>。</p>
<p><strong>情况 2：<span class="math inline">\(k &lt; 0\)</span>（例如
<span class="math inline">\(k = -1\)</span>）</strong><br />
缩放后的超平面参数为 <span class="math inline">\(\boldsymbol{w}&#39; =
(-1, 0)\)</span>，<span class="math inline">\(b&#39; =
0\)</span>，分类规则反转：<br />
如果 <span class="math inline">\(-x_1 &gt; 0 \quad \Rightarrow \quad x_1
&lt; 0\)</span>，则预测为 <span class="math inline">\(y =
+1\)</span>；<br />
如果 <span class="math inline">\(-x_1 &lt; 0 \quad \Rightarrow \quad x_1
&gt; 0\)</span>，则预测为 <span class="math inline">\(y =
-1\)</span>。</p>

</details>
<h3 id="定义">2.2. 定义</h3>
<h4 id="支持向量">2.2.1. 支持向量</h4>
<p><a href="#fig1">图1</a> 中，<span
class="math inline">\(H_1,H_{-1}\)</span>
经过的向量（一个蓝色，两个红色）即为支持向量。<br />
支持向量 <span class="math inline">\(\boldsymbol{x}_*\)</span>满足 <span
class="math display">\[\label{SV}
y_* \left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_*+b\right) = 1
\]</span></p>
<hr />
<p>根据 <a href="#SVM_purpose">SVM 的目标</a>，记超平面 <span
class="math inline">\(H_1\)</span> 与 <span
class="math inline">\(H_{-1}\)</span> 之间的距离为 <span
class="math inline">\(d\)</span>，我们很容易想到需要找到合适的 <span
class="math inline">\(H_1\)</span> 与 <span
class="math inline">\(H_{-1}\)</span> 使得 <span
class="math inline">\(d\)</span> 最大，此时超平面 <span
class="math inline">\(H_1\)</span> 与 <span
class="math inline">\(H_{-1}\)</span> 正中间的超平面 <span
class="math inline">\(H_0\)</span> 即为我们所求。<br />
为此我们需要定义关于距离 <span class="math inline">\(d\)</span>
的描述，也就是<strong>间隔 (margin)</strong>。</p>
<p><span id="I_think"></span>下面所说的超平面 <span
class="math inline">\((\boldsymbol{w},b)\)</span>
应该认为是已经能够将所有样本正确划分的了。<small><em>存疑</em></small></p>
<h4 id="函数间隔">2.2.2. 函数间隔</h4>
<p>对于给定的训练数据集 <span class="math inline">\(T\)</span> 和超平面
<span class="math inline">\((\boldsymbol{w},b)\)</span> ，定义超平面
<span class="math inline">\((\boldsymbol{w},b)\)</span> 关于样本点 <span
class="math inline">\((\boldsymbol{x}_i,y_i)\)</span> 的函数间隔为 <span
class="math display">\[\hat{\gamma}_i = y_i
\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i + b\right)\]</span></p>
<p>定义超平面 <span class="math inline">\((\boldsymbol{w},b)\)</span>
关于训练数据集 <span class="math inline">\(T\)</span> 的函数间隔为超平面
<span class="math inline">\((\boldsymbol{w},b)\)</span> 关于 <span
class="math inline">\(T\)</span> 中所有样本点 <span
class="math inline">\((\boldsymbol{x}_i,y_i)\)</span>
的函数间隔之最小值，即 <span class="math display">\[\hat{\gamma} =
\min_{i = 1,2,\dots,n}
\hat{\gamma}_i=y_*\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_*+b\right)\]</span>
其中 <span class="math inline">\(\boldsymbol{x}_*\)</span>
为支持向量。</p>
<p>函数间隔可以表示分类的正确性以及确信度。<br />
注意到函数间隔存在一些问题：当 <span
class="math inline">\(\boldsymbol{w},b\)</span>
成比例的改变时，虽然超平面没有改变，但函数间隔也会以同样的比例变化，因此不适合用于优化。</p>
<h4 id="几何间隔">2.2.3. 几何间隔</h4>
<p>对于给定的训练数据集 <span class="math inline">\(T\)</span> 和超平面
<span class="math inline">\((\boldsymbol{w},b)\)</span>，定义超平面
<span class="math inline">\((\boldsymbol{w},b)\)</span> 关于样本点 <span
class="math inline">\((\boldsymbol{x}_i,y_i)\)</span> 的几何间隔为 <span
class="math display">\[\gamma_i =
\frac{y_i\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i +
b\right)}{\|\boldsymbol{w}\|}\]</span></p>
<p>定义超平面 <span class="math inline">\((\boldsymbol{w},b)\)</span>
关于训练数据集 <span class="math inline">\(T\)</span> 的几何间隔为超平面
<span class="math inline">\((\boldsymbol{w},b)\)</span> 关于 <span
class="math inline">\(T\)</span> 中所有样本点 <span
class="math inline">\((\boldsymbol{x}_i,y_i)\)</span>
的几何间隔之最小值，即 <span class="math display">\[\gamma = \min_{i =
1,2,\dots,n}
\gamma_i=\frac{y_*\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_* +
b\right)}{\|\boldsymbol{w}\|}\]</span> 其中 <span
class="math inline">\(\boldsymbol{x}_*\)</span> 为支持向量。</p>
<p>超平面关于样本点的几何间隔指的就是点到超平面带符号的距离，当样本点被超平面分类正确的时候就是实际的距离了。</p>
<p>另外，几何间隔弥补了函数间隔存在的缺陷，即便 <span
class="math inline">\(\boldsymbol{w},b\)</span> 成比例的改变，但因为有了
<span class="math inline">\(\|\boldsymbol{w}\|\)</span>
这个分母项，使得几何间隔不变。<br />
也就是说几何间隔只与超平面的位置有关，比函数间隔更适合作为一个变量进行最优化。</p>
<p>根据函数间隔和几何间隔的定义，我们可以知道二者存在如下关系 <span
class="math display">\[
\begin{align}
\label{间隔关系1} \gamma_i = \frac{\hat{\gamma}_i}{\|\boldsymbol{w}\|}
\\
\label{间隔关系2} \gamma = \frac{\hat{\gamma}}{\|\boldsymbol{w}\|}
\end{align}
\]</span> 当 <span class="math inline">\(\|\boldsymbol{w}\|=1\)</span>
时，函数间隔和几何间隔二者相等。</p>
<h2 id="推导">3. 推导</h2>
<h3 id="目标函数">3.1. 目标函数</h3>
<p>由前文可知，我们需要最大化几何间隔，又称为硬间隔最大化。</p>
<p>间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p>
<p>因此我们考虑如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平面。用数学语言可以表示为如下的优化问题
<span class="math display">\[
\begin{equation}\label{opt}
\begin{aligned}
\max_{\boldsymbol{w},b} \quad&amp;\gamma = \max_{\boldsymbol{w},b} \min
\gamma_i \ ,\ i=1,2,\dots,n\\
\text{s.t.} \quad&amp; y_i
\frac{\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i + b}{\|\boldsymbol{w}\|}
\ge \gamma,\quad i=1,2,\dots,n
\end{aligned}
\end{equation}
\]</span></p>
<p>即最大化最小几何间隔，我们希望最大化超平面 <span
class="math inline">\((\boldsymbol{w},b)\)</span>
关于训练数据集的几何间隔 <span
class="math inline">\(\gamma\)</span>；约束条件表明，超平面 <span
class="math inline">\((\boldsymbol{w},b)\)</span>
关于每个训练样本的点的几何间隔至少为 <span
class="math inline">\(\gamma\)</span>。</p>
<p>考虑函数间隔和几何间隔的关系 <span
class="math inline">\(\eqref{间隔关系2}\)</span>，代入 <span
class="math inline">\(\eqref{opt}\)</span> 有 <span
class="math display">\[
\begin{align}
\max_{\boldsymbol{w},b} \quad&amp;
\frac{\hat{\gamma}}{\|\boldsymbol{w}\|}\\
\text{s.t.} \quad&amp; y_i
\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i + b\right) \ge
\hat{\gamma},\quad i=1,2,\dots,n
\end{align}
\]</span> 注意到，函数间隔 <span
class="math inline">\(\hat{\gamma}\)</span>
的取值并不影响最优化问题的解。例如 <span
class="math inline">\(\boldsymbol{w},b\)</span> 变为 <span
class="math inline">\(\lambda\boldsymbol{w},\lambda b\ (\lambda \gt
0)\)</span>，则有 <span class="math display">\[
\begin{equation}
\begin{aligned}
&amp;\frac{\hat{\gamma}}{\|\boldsymbol{w}\|}\rightarrow\frac{\lambda
\hat{\gamma}}{\lambda \|\boldsymbol{w}\|}\\
&amp;y_i \left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i + b\right) \ge
\hat{\gamma} \rightarrow y_i
\left(\left(\lambda\boldsymbol{w}\right)^\mathrm{T}\boldsymbol{x}_i +
\lambda b\right)=y_i \left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i +
b\right) \ge \lambda\hat{\gamma}
\end{aligned}
\end{equation}
\]</span></p>
<p>根据<strong>缩放不变性</strong>以及数据的线性可分性可知，取 <span
class="math inline">\(\lambda \hat{\gamma}=1\)</span>。</p>
<details class="note info"><summary><p>说明</p>
</summary>
<p>我还是认为从 B 站这个<a
href="https://b23.tv/UuJtZE5">讲解</a>的角度比较好理解<br />
从距离公式入手，结合缩放不变性 <span class="math display">\[\nonumber
d=\frac{|\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_0+b|}{\|\boldsymbol{w}\|}\quad
\xrightarrow{\text{经过缩放}} \frac{1}{\|\boldsymbol{w}\|}
\]</span> 这里其实比较纠结的是 <span
class="math inline">\(\hat{\gamma}\)</span> 的正负，导致对于 <span
class="math inline">\(\lambda\)</span> 正负的判断不清楚，因为实际上这里
<span class="math inline">\(\lambda\)</span> 只能取正的。<br />
所以我在前文<a
href="#I_think">引入间隔定义前</a>加了一句说明，认为这里的超平面已经能够将所有样本正确划分，才能够打消符号的疑虑。<br />
不过据说这本书 <a href="#ref3">[3]</a>
将的很明白，但是介于篇幅有点长，并且是英文的，暂时不想看。。。。</p>

</details>
<p>最终优化问题可以写为 <span class="math display">\[
\begin{equation}\label{final_opt}
\begin{aligned}
\min_{\boldsymbol{w},b} \quad&amp;\frac{1}{2}\|\boldsymbol{w}\|^2\\
\text{s.t.} \quad&amp; y_i
\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i + b\right) \ge 1,\quad
i=1,2,\dots,n
\end{aligned}
\end{equation}
\]</span></p>
<p>约束条件表明，每个样本都被正确分类。</p>
<h3 id="求解">3.2. 求解</h3>
<p>拉格朗日函数 <span class="math display">\[
\begin{equation}
\begin{aligned}
\mathcal{L}(\boldsymbol{w},b,\boldsymbol{\alpha})&amp;=\frac{1}{2}\|\boldsymbol{w}\|^2
+ \sum_{i = 1}^{n} \alpha_i(1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i +
b)),\quad \alpha_i \ge 0\\
&amp;=\frac{1}{2}\|\boldsymbol{w}\|^2 + \sum_{i = 1}^{n} (\alpha_i -
\alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i - \alpha_iy_ib)\\
&amp;=\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i = 1}^{n}
\alpha_i - \sum_{i = 1}^{n} \alpha_iy_i\boldsymbol{w}^T\boldsymbol{x}_i
- \sum_{i = 1}^{n} \alpha_iy_ib
\end{aligned}
\end{equation}
\]</span></p>
<p>对 <span class="math inline">\(\boldsymbol{w}\)</span> 和 <span
class="math inline">\(b\)</span> 分别求偏导数并令其为零</p>
<p><span class="math display">\[
\begin{align}
\label{diff_w}&amp;\frac{\partial L}{\partial\boldsymbol{w}}=\frac{1}{2}
\times 2\boldsymbol{w}+0 - \sum_{i = 1}^{n} \alpha_iy_i\boldsymbol{x}_i
- 0 = 0 \Longrightarrow \boldsymbol{w}=\sum_{i = 1}^{n}
\alpha_iy_i\boldsymbol{x}_i\\
\label{diff_b}&amp;\frac{\partial L}{\partial b}=0 + 0 - 0 - \sum_{i =
1}^{n} \alpha_iy_i = 0 \Longrightarrow \sum_{i = 1}^{n} \alpha_iy_i = 0
\end{align}
\]</span></p>
<p>将结果代回拉格朗日函数中消去 <span
class="math inline">\(\boldsymbol{w},b\)</span>，再考虑 <span
class="math inline">\(\eqref{diff_b}\)</span> 的约束条件，即可得到 <span
class="math inline">\(\eqref{final_opt}\)</span> 的对偶问题 <span
class="math display">\[
\begin{equation}\label{dual_problem}
\begin{aligned}
\max_{\boldsymbol{\alpha}}
\quad&amp;\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i
\alpha_j y_i y_j \boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_j\\
\text{s.t.} \quad&amp; \sum_{i=1}^n \alpha_i y_i=0,\quad \alpha_i \ge 0
\end{aligned}
\end{equation}
\]</span></p>
<p>解出 <span class="math inline">\(\alpha\)</span> 即可通过 <span
class="math inline">\(\eqref{diff_w}\)</span> 解出 <span
class="math inline">\(\boldsymbol{w}\)</span>，根据支持向量，代入 <span
class="math inline">\(\eqref{SV}\)</span> 中可得 <span
class="math display">\[
y_* \left(\sum_{i = 1}^{n}
\alpha_iy_i\boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_*+b\right) =
1\Longrightarrow b=y_* - \sum_{i = 1}^{n}
\alpha_iy_i\boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_*
\]</span></p>
<p>为了提高数值稳定性，考虑对全部支持向量的计算 <span
class="math inline">\(b\)</span> 并取平均值 <span
class="math display">\[
b=\frac{1}{N}\sum_{k=1}^{N}\left(y_k - \sum_{i = 1}^{n}
\alpha_iy_i\boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_k\right)
\]</span> 其中， <span class="math inline">\(N\)</span>
是支持向量的总数</p>
<details class="note info"><summary><p>补充 求解 <span
class="math inline">\(\eqref{dual_problem}\)</span></p>
</summary>
<p><strong>线性约束二次规划问题</strong> Linearly Constraint Quadratic
Programming<br />
<span class="math inline">\(\eqref{dual_problem}\)</span>
属于线性约束二次规划问题。<br />
该类型问题有两个特点：</p>
<ul>
<li>目标函数为二次项</li>
<li>约束条件为一次项</li>
</ul>
<p>该问题要么无解，要么只有一个极值。</p>
<p>求解 <span class="math inline">\(\eqref{dual_problem}\)</span>
可以用高效的 SMO 算法</p>
<p>todo</p>

</details>
<h3 id="核函数">3.3. 核函数</h3>
在前文中，我们始终是在数据集是线性可分的条件下讨论的，如 <a
href="#fig1">图1</a>，即存在一个超平面能够将不同类的样本完全分开。但是现实任务中在<strong>原始样本空间</strong>中或许并不存在一个能正确划分两类样本的超平面。例如如下这种“甜甜圈”形状的数据
<span id="fig2"></span>
<figure>
<img src="2d.svg" alt="2d" width="40%" height="40%" loading="lazy"/>
<figcaption>
图2. 非线性数据
</figcaption>
</figure>
对于这样的问题，我们可以将样本数据从原始空间映射到更高纬的线性空间，使得样本在这个特征空间内线性可分。例如将
<a href="#fig2">图2</a>
中的数据从原始的二维空间映射到一个合适的三维空间（如下图），那么就能找到一个合适的划分超平面。
<span id="fig3"></span>
<figure>
<img src="2d_kernel_3d.svg" alt="3d" width="80%" height="80%" loading="lazy"/>
<figcaption>
图3. 非线性映射
</figcaption>
</figure>
<p>幸运的是，根据 <a
href="https://en.wikipedia.org/wiki/Cover%27s_theorem">Cover 定理</a>
可知，<strong>如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分</strong>
（这里的"一定"指的是概率上趋近于 1）。</p>
<p><a href="#fig3">图3</a> 中 <span
class="math inline">\(\phi(\boldsymbol{x})\)</span> 表示将 <span
class="math inline">\(\boldsymbol{x}\)</span>
映射后的结果，则在该特征空间中对应的优化问题为 <span
class="math display">\[
\begin{equation}\label{kernel_opt}
\begin{aligned}
\min_{\boldsymbol{w},b} \quad&amp;\frac{1}{2}\|\boldsymbol{w}\|^2\\
\text{s.t.} \quad&amp; y_i
\left(\boldsymbol{w}^\mathrm{T}\phi\left(\boldsymbol{x}_i\right) +
b\right) - 1\ge 0,\quad i=1,2,\dots,n
\end{aligned}
\end{equation}
\]</span> 其对偶问题为 <span class="math display">\[
\begin{equation}\label{phi_dual}
\begin{aligned}
\max_{\boldsymbol{\alpha}}
\quad&amp;\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i
\alpha_j y_i
y_j\  \phi\left(\boldsymbol{x}_i\right)^\mathrm{T}\phi\left(\boldsymbol{x}_j\right)\\
\text{s.t.} \quad&amp; \sum_{i=1}^n \alpha_i y_i=0,\quad \alpha_i \ge 0
\end{aligned}
\end{equation}
\]</span></p>
<p>映射后的特征空间维度可能很高，为了避免显式计算映射 <span
class="math inline">\(\phi(\boldsymbol{x})\)</span>，引入核函数 <span
class="math inline">\(\kappa\left(\boldsymbol{x}_i,\boldsymbol{x}_j\right)\)</span>
直接计算高维空间的内积 <span
class="math inline">\(\phi\left(\boldsymbol{x}_i\right)^\mathrm{T}\phi\left(\boldsymbol{x}_j\right)\)</span>，即
<span class="math display">\[
\kappa\left(\boldsymbol{x}_i,\boldsymbol{x}_j\right)=\phi\left(\boldsymbol{x}_i\right)^\mathrm{T}\phi\left(\boldsymbol{x}_j\right)
\]</span></p>
<p>核函数通过直接利用原始数据 <span
class="math inline">\(\boldsymbol{x}_i,\boldsymbol{x}_j\)</span>
来计算映射到高维空间后的内积，从而避免因维度爆炸产生的高复杂度的计算以及存储问题。</p>
<p>因此 <span class="math inline">\(\eqref{phi_dual}\)</span> 可重写为
<span class="math display">\[
\begin{equation}\label{kernel_dual}
\begin{aligned}
\max_{\boldsymbol{\alpha}}
\quad&amp;\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i
\alpha_j y_i y_j
\  \kappa\left(\boldsymbol{x}_i,\boldsymbol{x}_j\right)\\
\text{s.t.} \quad&amp; \sum_{i=1}^n \alpha_i y_i=0,\quad \alpha_i \ge 0
\end{aligned}
\end{equation}
\]</span></p>
<p>只要一个对称函数所对应的核矩阵 <span class="math inline">\(K\)</span>
是半正定的，那它就能作为核函数，即 <span class="math display">\[
K=\begin{bmatrix}
\kappa\left(\boldsymbol{x}_1,\boldsymbol{x}_1\right) &amp; \dots &amp;
\kappa\left(\boldsymbol{x}_1,\boldsymbol{x}_j\right) &amp; \dots &amp;
\kappa\left(\boldsymbol{x}_1,\boldsymbol{x}_n\right)\\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\
\kappa\left(\boldsymbol{x}_i,\boldsymbol{x}_1\right) &amp; \dots &amp;
\kappa\left(\boldsymbol{x}_i,\boldsymbol{x}_j\right) &amp; \dots &amp;
\kappa\left(\boldsymbol{x}_i,\boldsymbol{x}_n\right)\\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\
\kappa\left(\boldsymbol{x}_n,\boldsymbol{x}_1\right) &amp; \dots &amp;
\kappa\left(\boldsymbol{x}_n,\boldsymbol{x}_j\right) &amp; \dots &amp;
\kappa\left(\boldsymbol{x}_n,\boldsymbol{x}_n\right)\\
\end{bmatrix}
\succeq 0
\]</span></p>
<p>下表列出了几个常用的核函数</p>
<table>
<caption>
表1. 常见核函数
</caption>
<tr>
<th>
名称
</th>
<th>
表达式
</th>
<th>
参数
</th>
</tr>
<tr>
<td>
线性核
</td>
<td>
<span class="math inline">\(\kappa(x_i, x_j) = x_i^T x_j\)</span>
</td>
<td>
</td>
</tr>
<tr>
<td>
多项式核
</td>
<td>
<span class="math inline">\(\kappa(x_i, x_j) = (x_i^T x_j)^d\)</span>
</td>
<td>
<span class="math inline">\(d \geq 1\)</span> 为多项式的次数
</td>
</tr>
<tr>
<td>
高斯核，RBF 核
</td>
<td>
<span class="math inline">\(\kappa(x_i, x_j) = \exp \left( -\frac{\|x_i
- x_j\|^2}{2\sigma^2} \right)\)</span>
</td>
<td>
<span class="math inline">\(\sigma &gt; 0\)</span> 为高斯核的带宽
</td>
</tr>
<tr>
<td>
拉普拉斯核
</td>
<td>
<span class="math inline">\(\kappa(x_i, x_j) = \exp \left( -\frac{\|x_i
- x_j\|}{\sigma} \right)\)</span>
</td>
<td>
<span class="math inline">\(\sigma &gt; 0\)</span>
</td>
</tr>
<tr>
<td>
Sigmoid核
</td>
<td>
<span class="math inline">\(\kappa(x_i, x_j) = \tanh (\beta x_i^T x_j +
\theta)\)</span>
</td>
<td>
<span class="math inline">\(\tanh\)</span> 为双曲正切函数，<span
class="math inline">\(\beta &gt; 0, \theta &lt; 0\)</span>
</td>
</tr>
</table>
<h3 id="软间隔">3.4. 软间隔</h3>
<p>前文讨论都是建立在数据在原始样本空间或特征空间中是线性可分的，即硬间隔，但是实际任务中，往往很难确定一个超平面将不同类的样本完全分开，即使恰好找到了某个核函数使得样本在特征空间中线性可分，也很难判定不是由过拟合造成的。</p>
解决该问题的一个办法是允许支持向量机在一些样本上出错，因此引入软间隔的概念
<span id="fig4"></span>
<figure>
<img src="soft_svm.svg" alt="soft_svm" width="60%" height="60%" loading="lazy"/>
<figcaption>
图4. 软间隔
</figcaption>
</figure>
<p>明显的可以看到训练数据中存在一些<strong>离群点
(outliers)</strong>，也就是米黄色背景的样本，除去这些离群点后剩下的大部分样本点组成的集合是线性可分的。对于这些离群点，它们显然不满足
<span class="math inline">\(\eqref{final_opt}\)</span> 中的约束条件
<span class="math inline">\(y_i
\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i + b\right)  \ge
1\)</span>。为了解决这个问题，对每个样本点引入一个<strong>松弛变量</strong>
<span class="math inline">\(\xi_i \ge
0\)</span>，使函数间隔加上松弛变量大于等于1。这样约束变为 <span
class="math display">\[
y_i\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+1\right) \ge 1-\xi_i
\]</span></p>
<p>同时对每个松弛变量支付一个代价。目标函数则变为 <span
class="math display">\[
\frac{1}{2}\|\boldsymbol{w}\|^2+C\sum_{i=1}^{n}\xi_i,\quad C \gt 0
\]</span> <span class="math inline">\(C\)</span> 为惩罚参数，<span
class="math inline">\(C\)</span>
越大，说明对误分类的惩罚越大；反之，对误分类的惩罚越小。</p>
<p>此时，支持向量机在软间隔下的优化问题可以描述为 <span
class="math display">\[
\begin{equation}\label{soft_opt}
\begin{aligned}
\min_{\boldsymbol{w},b,\xi}\quad &amp;
\frac{1}{2}\|\boldsymbol{w}\|^2+C\sum_{i=1}^{n}\xi_i\\
\text{s.t.}\quad &amp; y_i
\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i + b\right) \ge
1-\xi_i,\quad i=1,2,\dots,n\\
&amp; \xi_i \gt 0,\quad i=1,2,\dots,n
\end{aligned}
\end{equation}
\]</span> 最小化目标函数包含两层含义：使 <span
class="math inline">\(\frac{1}{2}\|\boldsymbol{w}\|^2\)</span>
尽可能小，即几何间隔尽可能大；同时使误分类的样本尽可能少。<span
class="math inline">\(C\)</span> 其实也相当于二者的调和系数。</p>
<p><span class="math inline">\(\eqref{spft_opt}\)</span> 的拉格朗日函数
<span class="math display">\[
\mathcal{L}\left(\boldsymbol{w},b,\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\mu}\right)=\frac{1}{2}\|\boldsymbol{w}\|^2
+ C\sum_{i=1}^{n}\xi_i +
\sum_{i=1}^{n}\alpha_i\left(1-\xi_i-y_i\left(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+b\right)\right)-\sum_{i=1}^{n}\mu_i\xi_i
\]</span> 其中，<span class="math inline">\(\xi_i \ge 0,\mu_i \ge
0\)</span>。</p>
<p>对偶问题为 <span class="math display">\[
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad &amp;-\frac{1}{2} \sum_{i = 1}^{n}
\sum_{j = 1}^{n} \alpha_i \alpha_j y_i y_j
\boldsymbol{x_i}^\mathrm{T}\boldsymbol{x_j} + \sum_{i = 1}^{n} \alpha_i
\\
\text{s.t.} \quad &amp;\sum_{i = 1}^{n} \alpha_i y_i = 0 \\
&amp; C - \alpha_i - \mu_i = 0 \\
&amp; \alpha_i \geq 0 \\
&amp; \mu_i \geq 0, \quad i = 1,2,\cdots,n
\end{aligned}
\]</span> 根据第二个等式约束条件消去 <span
class="math inline">\(\mu_i\)</span> 得 <span class="math display">\[
\begin{aligned}
\max_{\boldsymbol{\alpha}} \quad &amp;-\frac{1}{2} \sum_{i = 1}^{n}
\sum_{j = 1}^{n} \alpha_i \alpha_j y_i y_j
\boldsymbol{x_i}^\mathrm{T}\boldsymbol{x_j} + \sum_{i = 1}^{n} \alpha_i
\\
\text{s.t.} \quad &amp;\sum_{i = 1}^{n} \alpha_i y_i = 0 \\
&amp; 0\le\alpha_i \le C,\quad i = 1,2,\cdots,n
\end{aligned}
\]</span></p>
<details class="note info"><summary><p>SMO求解</p>
</summary>
<p>todo</p>

</details>
<h2 id="参考资料">4. 参考资料</h2>
<p><strong>[1]</strong>
李航著.机器学习方法[M].北京：清华大学出版社，2022.1<br />
<strong>[2]</strong> <a
href="https://blog.csdn.net/v_july_v/article/details/7624837">支持向量机通俗导论——理解SVM的三层境界</a><br />
<strong>[3]</strong> <a
href="https://timofey.pro/static/pdfdocs/AI_024_support_vector_machines_succinctly.pdf">Support
Vector Machines Succinctly</a><span id="ref3"></span><br />
<strong>[4]</strong> 周志华著.机器学习[M].北京：清华大学出版社，2016</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>SVM</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>非线性最小二乘</title>
    <url>/2025/01/15/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="前言">1. 前言</h2>
<p>先考虑一个最简单的最小二乘问题： <span class="math display">\[
\underset{x}{min} \frac{1}{2} \|f(\boldsymbol{x}) \|_{2}^{2}
\]</span> 这里自变量 <span class="math inline">\(\boldsymbol{x} \in
\mathbb{R}^n\)</span> ， <span class="math inline">\(f\)</span>
是任意一个非线性函数，假设他有 <span class="math inline">\(m\)</span>
维： <span class="math inline">\(f(\boldsymbol{x}) \in
\mathbb{R}^m\)</span> 。</p>
<p>考虑 <span class="math inline">\(f\)</span>
的形式基本有两种解决方法：解析法和迭代法。</p>
<h3 id="解析法">1.1 解析法</h3>
<p>当 <span class="math inline">\(f\)</span>
的数学形式比较简单的时候，或许问题可以直接用解析形式来求解。也就是让目标函数的导数为
0 ，然后求解 <span class="math inline">\(\boldsymbol{x}\)</span>
的最优值： <span class="math display">\[
\frac{\partial f}{\partial \boldsymbol{x}}=0 \nonumber
\]</span> 解上述方程即可得到目标函数导数为 0
的极值，它可能是极大值、极小值或者鞍点处的值，依次比较即可得到最小值。<br />
显然这种方法较为局限。</p>
<h3 id="迭代法">1.2 迭代法</h3>
<p>对于不易直接求导的目标函数，我们可以用迭代的方法来求极小值。根据给定的初值，不断地更新当前的优化变量，使得目标函数朝着下降的方向变化。具体步骤列写如下：</p>
<ol type="1">
<li>给定初值 <span class="math inline">\(\boldsymbol{x}_0\)</span>
；<br />
</li>
<li>对于第 <span class="math inline">\(k\)</span> 次迭代，寻找一个增量
<span class="math inline">\(\Delta \boldsymbol{x}_k\)</span>，使得 <span
class="math inline">\(\|f(\boldsymbol{x}_k + \Delta
\boldsymbol{x}_k)\|_{2}^{2}\)</span> 达到极小值；<br />
</li>
<li>如果 <span class="math inline">\(\Delta \boldsymbol{x}_k\)</span>
足够小，则停止；<br />
</li>
<li>否则，令 <span
class="math inline">\(\boldsymbol{x}_{k+1}=\boldsymbol{x}_k + \Delta
\boldsymbol{x}_k\)</span>，返回2。</li>
</ol>
<p>这样问题就转化为如何确定增量 <span class="math inline">\(\Delta
\boldsymbol{x}_k\)</span>，使上述过程收敛，通常有下面的几种方法。</p>
<h2 id="一阶和二阶梯度法最速下降法和牛顿法">2.
一阶和二阶梯度法（最速下降法和牛顿法）</h2>
<p>求解增量最直观的方法就是将 <strong>目标函数</strong> <span
id="GM"></span> 在 <span class="math inline">\(\boldsymbol{x}\)</span>
进行泰勒展开： <span class="math display">\[
\|f(\boldsymbol{x}+\Delta\boldsymbol{x})
\|_{2}^{2}=\|f(\boldsymbol{x})\|_{2}^{2}+\boldsymbol{J}(\boldsymbol{x})\Delta
\boldsymbol{x}+\frac{1}{2}\Delta \boldsymbol{x }^T \boldsymbol{H} \Delta
\boldsymbol{x}.
\]</span></p>
<p>这里 <span class="math inline">\(\boldsymbol{J}\)</span> 是 <span
class="math inline">\(\|f(\boldsymbol{x}) \|_{2}^{2}\)</span> 关于 <span
class="math inline">\(x\)</span> 的导数（雅可比矩阵），<span
class="math inline">\(\boldsymbol{H}\)</span>
则是二阶导数（黑塞矩阵）。我们可以选择保留泰勒展开的一阶或二阶项，对应的求解方法则为一阶梯度或二阶梯度法。</p>
<p>如果保留一阶梯度，那么增量的方向为： <span class="math display">\[
\Delta \boldsymbol{x}^{*}=-\boldsymbol{J}^T(\boldsymbol{x})
\]</span> 这里的转置是为了满足矩阵乘法的维度匹配。<br />
它的直观意义非常简单，只要我们沿着反向梯度方向前进即可。当然，我们还需要该方向上取一个步长
λ，求得最快的下降方式。这种方法被称为<strong>最速下降法</strong>。</p>
<p>如果保留二阶梯度，那么增量的解为： <span
class="math display">\[\label{二阶梯度法}
\boldsymbol{H} \Delta \boldsymbol{x}^{*}= - \boldsymbol{J}^T
\]</span> 该方法称又为<strong>牛顿法</strong>。</p>
<h3 id="小结">2.1 小结</h3>
<p>我们看到，一阶和二阶梯度法都十分直观，只要把函数在迭代点附近进行泰勒展开，并针对更新量作最小化即可。由于泰勒展开之后函数变成了多项式，所以求解增量时只需解线性方程即可，避免了直接求导函数为零这样的非线性方程的困难。</p>
<p>不过，这两种方法也存在它们自身的问题。最速下降法过于贪心，容易走出锯齿路线，反而增加了迭代次数。而牛顿法则需要计算目标函数的
<span class="math inline">\(\boldsymbol{H}\)</span>
矩阵，这在问题规模较大时非常困难，我们通常倾向于避免 <span
class="math inline">\(\boldsymbol{H}\)</span>
的计算。所以，接下来我们详细地介绍两类更加实用的方法： Gauss-Newton 和
Levenberg-Marquadt 。</p>
<h2 id="gauss-newton">3. Gauss-Newton</h2>
<p>Gauss Newton 是最优化算法里面最简单的方法之一。它的思想是将 <span
class="math inline">\(f(\boldsymbol{x})\)</span>
进行一阶的泰勒展开：<br />
<span class="math display">\[\label{局部近似}
f(\boldsymbol{x}+\Delta \boldsymbol{x}) \approx
f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})\Delta\boldsymbol{x}
\]</span> 这里 <span
class="math inline">\(\boldsymbol{J}(\boldsymbol{x})\)</span> 是 <span
class="math inline">\(f(\boldsymbol{x})\)</span> 关于 <span
class="math inline">\(\boldsymbol{x}\)</span> 的导数，实际上是一个 <span
class="math inline">\(m\times m\)</span>
的矩阵，也是一个雅可比矩阵。</p>
<div class="note danger"><p>这里是对 <span class="math inline">\(f(\boldsymbol{x})\)</span> 在
<span class="math inline">\(\boldsymbol{x}\)</span>
处进行泰勒展开，而不是对目标函数进行泰勒展开，需要和 <a
href="#GM">梯度法</a> 中进行区分对比！</p>
</div>
<p>根据前面的思想，我们当前的目标是为了寻找下降矢量 <span
class="math inline">\(\Delta\boldsymbol{x}\)</span>，使 <span
class="math inline">\(\|f(\boldsymbol{x}+\Delta\boldsymbol{x})\|_{2}^{2}\)</span>
达到最小。即<br />
<span class="math display">\[
\Delta \boldsymbol{x}^{*}=arg\  \underset{\Delta \boldsymbol{x}}{min}
\frac{1}{2}\|f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})\Delta\boldsymbol{x}\|_{2}^{2}
\]</span> 将目标函数展开化简得： <span class="math display">\[
\begin{equation*}
\begin{aligned}
\frac{1}{2}\|f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})\Delta\boldsymbol{x}\|_{2}^{2}&amp;=\frac{1}{2}(f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})\Delta\boldsymbol{x})^T(f(\boldsymbol{x})+\boldsymbol{J}(\boldsymbol{x})\Delta\boldsymbol{x})
\\
&amp;=\frac{1}{2}(\|f(\boldsymbol{x})\|_{2}^{2} +
2f(\boldsymbol{x})^T\boldsymbol{J}(\boldsymbol{x}+\Delta\boldsymbol{x}^T
\boldsymbol{J}(\boldsymbol{x})^T\boldsymbol{J}(x)^T\Delta
\boldsymbol{x}))
\end{aligned}
\end{equation*}
\]</span></p>
<p>求上述目标函数对 <span class="math inline">\(\Delta
\boldsymbol{x}\)</span> 的导数，并令其为 0： <span
class="math display">\[\nonumber
2\boldsymbol{J}(\boldsymbol{x})^T \boldsymbol{J}(\boldsymbol{x})\Delta
\boldsymbol{x}+2\boldsymbol{J}(\boldsymbol{x})^T f(\boldsymbol{x})=0
\]</span></p>
<p>可以得到： <span class="math display">\[
\boldsymbol{J}(\boldsymbol{x})^T \boldsymbol{J}(\boldsymbol{x})\Delta
\boldsymbol{x}=-\boldsymbol{J}(\boldsymbol{x})^T f(\boldsymbol{x})
\]</span></p>
<p>注意，我们要求解的变量是 <span class="math inline">\(\Delta
\boldsymbol{x}\)</span>，因此这是一个线性方程组，我们称它为<strong>增量方程</strong>，也可以称为<strong>高斯牛顿方程</strong>
(Gauss Newton equations) 或者<strong>正规方程</strong> (Normal
equations)。我们把左边的系数定义为 <span
class="math inline">\(\boldsymbol{H}\)</span>，右边定义为 <span
class="math inline">\(\boldsymbol{g}\)</span>，那么上式变为：<br />
<span class="math display">\[
\boldsymbol{H}\Delta \boldsymbol{x} = \boldsymbol{g}
\]</span></p>
<p>Gauss-Newton 的步骤可以写为：</p>
<ol type="1">
<li>给定初值 <span
class="math inline">\(\boldsymbol{x}_0\)</span>；<br />
</li>
<li>对于第 <span class="math inline">\(k\)</span>
次迭代，求出当前的雅可比矩阵 <span
class="math inline">\(\boldsymbol{J}(\boldsymbol{x}_k)\)</span>
和误差<span class="math inline">\(f(\boldsymbol{x}_k)\)</span>；<br />
</li>
<li>求解增量方程： <span class="math inline">\(\boldsymbol{H}\Delta
\boldsymbol{x}_k = \boldsymbol{g}\)</span> ；<br />
</li>
<li>如果 <span class="math inline">\(\Delta \boldsymbol{x}_k\)</span>
足够小，则停止；否则，令 <span
class="math inline">\(\boldsymbol{x}_{k+1}=\boldsymbol{x}_k+\Delta
\boldsymbol{x}_k\)</span> ，返回 2。</li>
</ol>
<p><strong>求解增量方程是整个优化问题的核心所在</strong>。</p>
<h3 id="小结-1">3.1 小结</h3>
<p>和 二阶梯度法（牛顿法）<span
class="math inline">\(\eqref{二阶梯度法}\)</span>
的增量解进行对比，可以发现 Gauss-Newton 中用 <span
class="math inline">\(\boldsymbol{J}^{T}\boldsymbol{J}\)</span>
作为牛顿法中二阶 Hessian 矩阵的近似，从而省略了计算 <span
class="math inline">\(\boldsymbol{H}\)</span> 的过程。</p>
<p>Gauss-Newton 的核心在于求解增量方程，但它要求 <span
class="math inline">\(\boldsymbol{H}\)</span>
是可逆且正定的，但实际中计算的 <span
class="math inline">\(\boldsymbol{H}=\boldsymbol{J}^{T}\boldsymbol{J}\)</span>
通常只是半正定的。因此，在使用 Gauss-Newton 的时候可能会出现 <span
class="math inline">\(\boldsymbol{H}\)</span>
是奇异或者病态的情况，导致算法不收敛。更严重的是，就算我们假设 <span
class="math inline">\(\boldsymbol{H}\)</span>
非奇异也非病态，如果我们求出来的步长 <span class="math inline">\(\Delta
\boldsymbol{x}\)</span> 太大，也会导致我们采用的局部近似 <span
class="math inline">\(\eqref{局部近似}\)</span>
不够准确，这样一来我们甚至都无法保证它的迭代收敛，哪怕是让目标函数变得更大都是有可能的。</p>
<p>尽管 Gauss-Newton
有这些缺点，但是它依然值得我们去学习，因为在非线性优化里，相当多的算法都可以归结为
Gauss-Newton 的变种。这些算法都借助了 Gauss-Newton
法的思想并且通过自己的改进修正 Gauss-Newton 法的缺点。例如一些线搜索方法
(line search method)，这类改进就是加入了一个标量 <span
class="math inline">\(\alpha\)</span>，在确定了 <span
class="math inline">\(\Delta \boldsymbol{x}\)</span> 进一步找到 <span
class="math inline">\(\alpha\)</span> 使得 <span
class="math inline">\(\|f(\boldsymbol{x}+\alpha \Delta
\boldsymbol{x})\|_{2}^{2}\)</span> 达到最小，而不是像 Gauss-Newton
法那样简单地令 <span class="math inline">\(α = 1\)</span>。</p>
<p>Levenberg-Marquadt
方法在一定程度上修正了这些问题，被称之为阻尼牛顿法(Damped Newton
Method)，一般认为它比高斯牛顿更为鲁棒。尽管它的收敛速度可能会比高斯牛顿更慢，但是在
SLAM 里面却被大量应用。</p>
<h2 id="levenberg-marquadt">4. Levenberg-Marquadt</h2>
<p>前文提到当 <span class="math inline">\(\Delta \boldsymbol{x}\)</span>
太大时，Gauss-Newton 方法中采用的近似二阶泰勒展开 <span
class="math inline">\(\eqref{局部近似}\)</span>
存在不够准确的问题，很自然的想到对 <span class="math inline">\(\Delta
\boldsymbol{x}\)</span> 增加一个信赖区域 (Trust Region)，不能让 <span
class="math inline">\(\Delta \boldsymbol{x}\)</span>
因太大而使得近似不够准却。这种添加信赖区域的方法在非线性优化中被称为
<strong>信赖区域方法 (Trust Region Method)</strong>
。只有在规定的信赖区域中的 <span class="math inline">\(\Delta
\boldsymbol{x}\)</span> 才认为近似是有效的。<br />
那么如何确定这个信赖区域的范围呢？一个比较好的方法是根据我们的近似模型跟实际函数之间的差异来确定这个范围：如果差异小，我们就让范围尽可能大；如果差异大，我们就缩小这个近似范围。因此，考虑使用下式来判断泰勒近似是否够好。
<span class="math display">\[
\rho=\frac{f(\boldsymbol{x+\Delta
\boldsymbol{x}})-f(\boldsymbol{x})}{\boldsymbol{J}(\boldsymbol{x})\Delta
\boldsymbol{x}}=\frac{实际函数下降值}{近似模型下降值}
\]</span> 显然，当 <span class="math inline">\(\rho \rightarrow
1\)</span> 时，近似效果最好；<br />
当 <span class="math inline">\(\rho \ll 1\)</span>
时，说明实际下降值远小于近似下降值，我们需要缩小近似范围，即减小 <span
class="math inline">\(\Delta \boldsymbol{x}\)</span> ；<br />
当 <span class="math inline">\(\rho \gg 1\)</span>
时，说明实际下降值远大于近似下降值，我们需要增大近似范围，即增大 <span
class="math inline">\(\Delta \boldsymbol{x}\)</span> 。</p>
<p>依此，我们可以改进 Gauss-Newton 非线性优化的框架：</p>
<ol type="1">
<li>给定初始值 <span
class="math inline">\(\boldsymbol{x}_0\)</span>，以及初始优化半径 <span
class="math inline">\(\mu\)</span>；</li>
<li>对于第 <span class="math inline">\(k\)</span> 次迭代，求解：<span
class="math display">\[\label{L-M} \underset{\Delta
\boldsymbol{x}_k}{min}
\frac{1}{2}\|f(\boldsymbol{x}_k)+\boldsymbol{J}(\boldsymbol{x}_k)\Delta
\boldsymbol{x}_k\|_{2}^{2},\ s.t.\|\boldsymbol{D}\Delta
\boldsymbol{x}_k\|^{2}\le \mu\]</span> 这里 <span
class="math inline">\(\mu\)</span> 是信赖区域的半径，<span
class="math inline">\(\boldsymbol{D}\)</span> 是一个调整矩阵</li>
<li>计算 <span class="math inline">\(\rho\)</span>；</li>
<li>如果 <span class="math inline">\(\rho \gt \frac{3}{4}\)</span>，则
<span class="math inline">\(\mu = 2\mu\)</span>；如果 <span
class="math inline">\(\rho \lt \frac{1}{4}\)</span>，则 <span
class="math inline">\(\mu = 0.5\mu\)</span>；否则，<span
class="math inline">\(\mu\)</span>
保持不变。这里的阈值和扩大倍数都是经验值，可视情况调整；</li>
<li>如果 <span class="math inline">\(\rho\)</span>
大于某阈值，认为近似可行。令 <span
class="math inline">\(\boldsymbol{x}_{k+1}=\boldsymbol{x}_k+\Delta
\boldsymbol{x}_k\)</span>；</li>
<li>判断算法是否收敛。如果不收敛则返回2，否则结束。</li>
</ol>
<p>关于 <span class="math inline">\(\eqref{L-M}\)</span> 中 <span
class="math inline">\(\boldsymbol{D}\)</span> 的说明：<br />
当 <span class="math inline">\(\boldsymbol{D}=\boldsymbol{I}\)</span>
时，这里的约束条件可看作是将 <span class="math inline">\(\Delta
\boldsymbol{x}\)</span> 约束在一个半径为 <span
class="math inline">\(\mu\)</span> 的球内；当 <span
class="math inline">\(\boldsymbol{D} \ne \boldsymbol{I}\)</span>
时，可以认为这是一个椭球。 在 L-M 算法中，取 <span
class="math inline">\(\boldsymbol{D}\)</span> 为非负对角矩阵（由 <span
class="math inline">\(\boldsymbol{J}^{T}\boldsymbol{J}\)</span>
的对角元素平方根构成），使得在梯度小的维度上约束范围更大一些。</p>
<p>根据<span class="math inline">\(\eqref{L-M}\)</span> 求解增量，
拉格朗日法： <span class="math display">\[
\nonumber
L=\frac{1}{2}\|f(\boldsymbol{x}_k)+\boldsymbol{J}(\boldsymbol{x}_k)\Delta
\boldsymbol{x}_k\|_{2}^{2}+\frac{\lambda}{2}\|\boldsymbol{D}\Delta\boldsymbol{x}_k\|_{2}^{2},\lambda
\gt 0
\]</span> 令偏导为0，可得增量方程： <span class="math display">\[
\frac{\partial L}{\partial \Delta\boldsymbol{x}_k}=\boldsymbol{0}
\Longrightarrow (\boldsymbol{H}+\lambda
\boldsymbol{D}^{T}\boldsymbol{D})\Delta \boldsymbol{x}_k=\boldsymbol{g}
\]</span></p>
<p>从结果来看，当 <span class="math inline">\(\lambda\)</span>
较小时，<span class="math inline">\(\boldsymbol{H}\)</span>
占主导地位，说明二次近似模型效果在该范围内是比较好的，此时 L-M 法接近于
G-N 法；当 <span class="math inline">\(\lambda\)</span> 较大时，考虑
<span class="math inline">\(\boldsymbol{D}=\boldsymbol{I}\)</span>
的简化形式，此时 <span class="math inline">\(\lambda \boldsymbol{D}^T
\boldsymbol{D}=\lambda \boldsymbol{I}\)</span>
占主导地位，说明二次近似的不够好，此时 L-M 法更接近于
一阶梯度下降法（最速下降法）。</p>
<p>L-M
的求解方式，可在一定程度上避免线性方程组的系数矩阵的非奇异和病态问题，提供更稳定更准确的增量
<span class="math inline">\(\Delta \boldsymbol{x}\)</span>。</p>
<p>总而言之，非线性优化问题的框架，分为 Line Search 和 Trust Region
两类。<br />
Line Search 先固定搜索方向，然后在该方向寻找步长，以最速下降法和
Gauss-Newton 法为代表。而 TrustRegion
则先固定搜索区域，再考虑找该区域内的最优点。此类方法以 L-M
为代表。实际问题中，我们通常选择 G-N 或 L-M 之一作为梯度下降策略。</p>
<h2 id="参考资料">5. 参考资料</h2>
<p><strong>[1]</strong> 视觉 SLAM 十四讲 —— 高翔</p>
]]></content>
      <categories>
        <category>最优化</category>
        <category>非线性最小二乘</category>
      </categories>
      <tags>
        <tag>Levenberg-Marquadt</tag>
      </tags>
  </entry>
</search>
